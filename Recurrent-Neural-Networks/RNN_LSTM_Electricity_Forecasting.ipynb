{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5348d973",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with RNN and LSTM (time-series)\n",
    "\n",
    "## üìä Electricity Consumption Forecasting with Deep Learning\n",
    "\n",
    "This comprehensive notebook performs **Exploratory Data Analysis (EDA)** for electricity consumption forecasting using **Recurrent Neural Networks (RNN)** and **Long Short-Term Memory (LSTM)** networks with **PyTorch**.\n",
    "\n",
    "### **Overview**\n",
    "- **Dataset**: Italian electricity consumption (2024) + Temperature data from Lazio region\n",
    "- **Goal**: Forecast daily electricity demand using deep learning time-series models\n",
    "- **Approach**: Multi-feature RNN/LSTM with weather integration\n",
    "- **Region**: Center-North Italy, focusing on Lazio (Roma area)\n",
    "\n",
    "### **Analysis**\n",
    "1. **Data Quality Assessment** - Missing values, outliers, data consistency\n",
    "2. **Temperature-Load Correlation** - Understanding the U-shaped relationship\n",
    "3. **Seasonal Pattern Analysis** - Winter heating vs summer cooling effects\n",
    "4. **Feature Engineering** - Creating temporal and seasonal features for deep learning\n",
    "5. **Data Preprocessing** - Normalization and sequence preparation for RNN/LSTM\n",
    "\n",
    "### üß† **Deep Learning pipeline**\n",
    "```\n",
    "Raw Data ‚Üí EDA ‚Üí Feature Engineering ‚Üí Sequence Creation ‚Üí RNN/LSTM Training ‚Üí Forecasting\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d97bc4",
   "metadata": {},
   "source": [
    "## 1. Setup Python Virtual Environment\n",
    "\n",
    "Setting up an isolated Python environment for our electricity forecasting project to ensure consistent dependencies and avoid conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Virtual Environment Setup:\n",
      "==================================================\n",
      "1. Create virtual environment:\n",
      "   python3 -m venv electricity_forecast_env\n",
      "\n",
      "2. Activate virtual environment:\n",
      "   Linux/Mac: source electricity_forecast_env/bin/activate\n",
      "   Windows:   electricity_forecast_env\\Scripts\\activate\n",
      "\n",
      "3. Install dependencies:\n",
      "   pip install -r requirements.txt\n",
      "\n",
      "4. Start Jupyter notebook:\n",
      "   jupyter notebook\n",
      "==================================================\n",
      "üìç Current Python executable: /home/laptop/EXERCISES/DEEP-LEARNING/PYTORCH/Deep-Learning-and-PyTorch/Recurrent-Neural-Networks/electricity_forecast_env/bin/python\n",
      "üìÅ Current working directory: /home/laptop/EXERCISES/DEEP-LEARNING/PYTORCH/Deep-Learning-and-PyTorch/Recurrent-Neural-Networks\n",
      "üî¢ Python version: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]\n",
      "‚úÖ Virtual environment is ACTIVE\n"
     ]
    }
   ],
   "source": [
    "# Virtual Environment Setup Commands\n",
    "# These commands should be run in the terminal before starting Jupyter\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"üîß Virtual Environment Setup:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Create virtual environment:\")\n",
    "print(\"   python3 -m venv electricity_forecast_env\")\n",
    "print()\n",
    "print(\"2. Activate virtual environment:\")\n",
    "print(\"   Linux/Mac: source electricity_forecast_env/bin/activate\")\n",
    "print(\"   Windows:   electricity_forecast_env\\\\Scripts\\\\activate\")\n",
    "print()\n",
    "print(\"3. Install dependencies:\")\n",
    "print(\"   pip install -r requirements.txt\")\n",
    "print()\n",
    "print(\"4. Start Jupyter notebook:\")\n",
    "print(\"   jupyter notebook\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check current Python environment\n",
    "print(f\" Current Python executable: {sys.executable}\")\n",
    "print(f\"üìÅ Current working directory: {os.getcwd()}\")\n",
    "print(f\" Python version: {sys.version}\")\n",
    "\n",
    "# Verify we're in virtual environment\n",
    "if 'electricity_forecast_env' in sys.executable:\n",
    "    print(\"Python Virtual environment is ACTIVE\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Virtual environment might not be active\")\n",
    "    print(\"   Please ensure you activated the virtual environment before starting Jupyter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73812a5",
   "metadata": {},
   "source": [
    "## 2. Install required dependencies\n",
    "\n",
    "Installing all necessary Python packages for deep learning, data analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Checking Required Dependencies for RNN/LSTM Electricity Forecasting:\n",
      "======================================================================\n",
      "‚úÖ torch           - Version: 2.7.1+cu126\n",
      "‚úÖ numpy           - Version: 2.3.1\n",
      "‚úÖ pandas          - Version: 2.3.1\n",
      "‚úÖ matplotlib      - Version: 3.10.3\n",
      "‚úÖ seaborn         - Version: 0.13.2\n",
      "‚ùå scikit-learn    - NOT INSTALLED\n",
      "‚úÖ scipy           - Version: 1.16.0\n",
      "‚úÖ plotly          - Version: 6.2.0\n",
      "‚úÖ flask           - Version: 3.1.1\n",
      "‚úÖ requests        - Version: 2.32.4\n",
      "======================================================================\n",
      "üìä Summary: 9/10 packages installed\n",
      "\n",
      "‚ö†Ô∏è  Missing packages: scikit-learn\n",
      "üîß Install missing packages with:\n",
      "   pip install scikit-learn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75353/2662217409.py:29: DeprecationWarning: The '__version__' attribute is deprecated and will be removed in Flask 3.2. Use feature detection or 'importlib.metadata.version(\"flask\")' instead.\n",
      "  version = getattr(module, '__version__', 'Unknown')\n"
     ]
    }
   ],
   "source": [
    "# Install and verify required packages\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "# List of required packages for electricity forecasting\n",
    "required_packages = [\n",
    "    'torch',\n",
    "    'numpy', \n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn',\n",
    "    'scipy',\n",
    "    'plotly',\n",
    "    'flask',\n",
    "    'requests'\n",
    "]\n",
    "\n",
    "print(\"Checking Required Dependencies for RNN/LSTM Electricity Forecasting:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "missing_packages = []\n",
    "installed_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        # Try to import the package\n",
    "        module = importlib.import_module(package)\n",
    "        version = getattr(module, '__version__', 'Unknown')\n",
    "        print(f\"{package:<15} - Version: {version}\")\n",
    "        installed_packages.append(package)\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package:<15} - NOT INSTALLED\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìä Summary: {len(installed_packages)}/{len(required_packages)} packages installed\")\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"üîß Install missing packages with:\")\n",
    "    print(f\"   pip install {' '.join(missing_packages)}\")\n",
    "else:\n",
    "    print(\"\\n All required packages are installed!\")\n",
    "    print(\"Ready for electricity consumption analysis with RNN/LSTM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1afdb0",
   "metadata": {},
   "source": [
    "## 3. Import Python libraries\n",
    "\n",
    "Importing all required Python libraries for data analysis, visualization, and machine learning preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba60fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Libraries imported successfully!\n",
      "üî• PyTorch version: 2.7.1+cu126\n",
      "üìä Pandas version: 2.3.1\n",
      "üî¢ NumPy version: 2.3.1\n",
      "üìà Matplotlib version: 3.10.3\n",
      "üé® Seaborn version: 0.13.2\n",
      "üöÄ CUDA available: NVIDIA GeForce MX450\n",
      "\n",
      "‚úÖ Ready for Electricity Consumption Analysis with RNN/LSTM!\n"
     ]
    }
   ],
   "source": [
    "# Core Data Analysis Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Interactive Plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Matplotlib settings for better plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "\n",
    "# Check if CUDA is available for PyTorch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU for PyTorch operations\")\n",
    "\n",
    "print(\"\\nReady for Electricity Consumption Analysis with RNN/LSTM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d947a5",
   "metadata": {},
   "source": [
    "## 4. ‚ö° Load electricity consumption dataset\n",
    "\n",
    "Loading and examining the Italian electricity consumption data from the Center-North region (2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77885944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load electricity consumption dataset\n",
    "electricity_file_path = 'Dataset/electrical-consumption-2024.csv'\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    electricity_df = pd.read_csv(electricity_file_path)\n",
    "    \n",
    "    print(\"‚ö° ELECTRICITY CONSUMPTION DATASET LOADED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìÅ File: {electricity_file_path}\")\n",
    "    print(f\"Shape: {electricity_df.shape} (rows √ó columns)\")\n",
    "    print(f\"Data period: 2024 (Center-North Italy)\")\n",
    "    print(f\"Region: Center-North Italy (including Lazio)\")\n",
    "    print(f\"Frequency: 15-minute intervals (will aggregate to daily)\")\n",
    "    \n",
    "    print(\"\\nDATASET STRUCTURE:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(electricity_df.info())\n",
    "    \n",
    "    print(\"\\nüìã COLUMN NAMES:\")\n",
    "    print(\"=\" * 20)\n",
    "    for i, col in enumerate(electricity_df.columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    print(\"\\nüëÄ FIRST 5 ROWS:\")\n",
    "    print(\"=\" * 20)\n",
    "    display(electricity_df.head())\n",
    "    \n",
    "    print(\"\\nDATA TYPES:\")\n",
    "    print(\"=\" * 15)\n",
    "    print(electricity_df.dtypes)\n",
    "    \n",
    "    print(\"\\nKEY COLUMN FOR ANALYSIS:\")\n",
    "    if 'Total Load [MW]' in electricity_df.columns:\n",
    "        print(\"'Total Load [MW]' column found - this is our target variable\")\n",
    "        print(f\"Load range: {electricity_df['Total Load [MW]'].min():.2f} - {electricity_df['Total Load [MW]'].max():.2f} MW\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  'Total Load [MW]' column not found. Available columns:\")\n",
    "        print(list(electricity_df.columns))\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: File '{electricity_file_path}' not found!\")\n",
    "    print(\"üìÅ Please ensure the Dataset folder contains 'electrical-consumption-2024.csv'\")\n",
    "    print(\"üìÇ Expected structure:\")\n",
    "    print(\"   Dataset/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ electrical-consumption-2024.csv\")\n",
    "    print(\"   ‚îî‚îÄ‚îÄ temperature-2024.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading electricity dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea57f8",
   "metadata": {},
   "source": [
    "## 5. üå°Ô∏è Load temperature dataset\n",
    "\n",
    "Loading and exploring the temperature data from Roma, Lazio (representative of Center-North region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load temperature dataset\n",
    "temperature_file_path = 'Dataset/temperature-2024.csv'\n",
    "\n",
    "try:\n",
    "    # Load the temperature dataset\n",
    "    temperature_df = pd.read_csv(temperature_file_path)\n",
    "    \n",
    "    print(\"TEMPERATURE DATASET LOADED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìÅ File: {temperature_file_path}\")\n",
    "    print(f\"Shape: {temperature_df.shape} (rows √ó columns)\")\n",
    "    print(f\"Data period: 2024\")\n",
    "    print(f\"Location: Roma, Lazio, Italia\")\n",
    "    print(f\"Frequency: Daily\")\n",
    "    \n",
    "    print(\"\\nDATASET STRUCTURE:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(temperature_df.info())\n",
    "    \n",
    "    print(\"\\nüìã COLUMN NAMES:\")\n",
    "    print(\"=\" * 20)\n",
    "    for i, col in enumerate(temperature_df.columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    print(\"\\nüëÄ FIRST 5 ROWS:\")\n",
    "    print(\"=\" * 20)\n",
    "    display(temperature_df.head())\n",
    "    \n",
    "    print(\"\\nDATA TYPES:\")\n",
    "    print(\"=\" * 15)\n",
    "    print(temperature_df.dtypes)\n",
    "    \n",
    "    print(\"\\nKEY COLUMN FOR ANALYSIS:\")\n",
    "    if 'temp' in temperature_df.columns:\n",
    "        print(\"'temp' column found - this is our temperature predictor\")\n",
    "        print(f\"Temperature range: {temperature_df['temp'].min():.1f}¬∞C - {temperature_df['temp'].max():.1f}¬∞C\")\n",
    "        print(f\"Average temperature: {temperature_df['temp'].mean():.1f}¬∞C\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  'temp' column not found. Available columns:\")\n",
    "        print(list(temperature_df.columns))\n",
    "    \n",
    "    # Check for date column\n",
    "    date_columns = [col for col in temperature_df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "    if date_columns:\n",
    "        print(f\"\\nüìÖ Date column(s) found: {date_columns}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No obvious date column found\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: File '{temperature_file_path}' not found!\")\n",
    "    print(\"üìÅ Please ensure the Dataset folder contains 'temperature-2024.csv'\")\n",
    "    print(\"üìÇ Expected structure:\")\n",
    "    print(\"   Dataset/\")\n",
    "    print(\"   ‚îú‚îÄ‚îÄ electrical-consumption-2024.csv\")\n",
    "    print(\"   ‚îî‚îÄ‚îÄ temperature-2024.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading temperature dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844dd47",
   "metadata": {},
   "source": [
    "## 6. Data cleaning and preprocessing\n",
    "\n",
    "Handling missing values, outliers, and data quality issues in both electricity and temperature datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9aed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Quality Assessment\n",
    "print(\"üßπ DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Function to analyze data quality\n",
    "def analyze_data_quality(df, dataset_name):\n",
    "    print(f\"\\nüìä {dataset_name} DATA QUALITY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"‚ùì Missing values:\")\n",
    "    if missing_values.sum() == 0:\n",
    "        print(\"   ‚úÖ No missing values found\")\n",
    "    else:\n",
    "        for col, missing in missing_values.items():\n",
    "            if missing > 0:\n",
    "                percentage = (missing / len(df)) * 100\n",
    "                print(f\"   ‚ö†Ô∏è  {col}: {missing} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Duplicated rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"üîÑ Duplicated rows: {duplicates}\")\n",
    "    if duplicates == 0:\n",
    "        print(\"   ‚úÖ No duplicated rows found\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Found {duplicates} duplicated rows\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"üìè Data types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"   {col}: {dtype}\")\n",
    "    \n",
    "    return missing_values, duplicates\n",
    "\n",
    "# Analyze electricity dataset\n",
    "if 'electricity_df' in locals():\n",
    "    elec_missing, elec_duplicates = analyze_data_quality(electricity_df, \"ELECTRICITY\")\n",
    "    \n",
    "    # Check for outliers in Total Load\n",
    "    if 'Total Load [MW]' in electricity_df.columns:\n",
    "        load_col = 'Total Load [MW]'\n",
    "        Q1 = electricity_df[load_col].quantile(0.25)\n",
    "        Q3 = electricity_df[load_col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = electricity_df[(electricity_df[load_col] < lower_bound) | \n",
    "                                 (electricity_df[load_col] > upper_bound)]\n",
    "        \n",
    "        print(f\"\\nüéØ ELECTRICITY LOAD OUTLIER ANALYSIS:\")\n",
    "        print(f\"   üìä Q1: {Q1:.2f} MW\")\n",
    "        print(f\"   üìä Q3: {Q3:.2f} MW\")\n",
    "        print(f\"   üìä IQR: {IQR:.2f} MW\")\n",
    "        print(f\"   üìä Lower bound: {lower_bound:.2f} MW\")\n",
    "        print(f\"   üìä Upper bound: {upper_bound:.2f} MW\")\n",
    "        print(f\"   üîç Outliers found: {len(outliers)} ({len(outliers)/len(electricity_df)*100:.1f}%)\")\n",
    "\n",
    "# Analyze temperature dataset\n",
    "if 'temperature_df' in locals():\n",
    "    temp_missing, temp_duplicates = analyze_data_quality(temperature_df, \"TEMPERATURE\")\n",
    "    \n",
    "    # Check for outliers in temperature\n",
    "    if 'temp' in temperature_df.columns:\n",
    "        temp_col = 'temp'\n",
    "        Q1_temp = temperature_df[temp_col].quantile(0.25)\n",
    "        Q3_temp = temperature_df[temp_col].quantile(0.75)\n",
    "        IQR_temp = Q3_temp - Q1_temp\n",
    "        lower_bound_temp = Q1_temp - 1.5 * IQR_temp\n",
    "        upper_bound_temp = Q3_temp + 1.5 * IQR_temp\n",
    "        \n",
    "        temp_outliers = temperature_df[(temperature_df[temp_col] < lower_bound_temp) | \n",
    "                                      (temperature_df[temp_col] > upper_bound_temp)]\n",
    "        \n",
    "        print(f\"\\nüå°Ô∏è TEMPERATURE OUTLIER ANALYSIS:\")\n",
    "        print(f\"   üìä Q1: {Q1_temp:.1f}¬∞C\")\n",
    "        print(f\"   üìä Q3: {Q3_temp:.1f}¬∞C\")\n",
    "        print(f\"   üìä IQR: {IQR_temp:.1f}¬∞C\")\n",
    "        print(f\"   üìä Lower bound: {lower_bound_temp:.1f}¬∞C\")\n",
    "        print(f\"   üìä Upper bound: {upper_bound_temp:.1f}¬∞C\")\n",
    "        print(f\"   üîç Outliers found: {len(temp_outliers)} ({len(temp_outliers)/len(temperature_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data quality analysis completed!\")\n",
    "print(\"üéØ Next step: Merge datasets by date for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712ba6e",
   "metadata": {},
   "source": [
    "## 10. üìà Correlation analysis between temperature and load\n",
    "\n",
    "**The Core of RNN/LSTM forecasting: Understanding temperature-electricity relationships**\n",
    "\n",
    "This analysis reveals the U-shaped correlation that makes temperature a powerful predictor for electricity demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b451561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature-Load Correlation Analysis for RNN/LSTM\n",
    "print(\"TEMPERATURE-LOAD CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This analysis is crucial for understanding why RNN/LSTM models\")\n",
    "print(\"   use both electricity and temperature data together\")\n",
    "print()\n",
    "\n",
    "# Simulate correlation analysis (replace with actual data when available)\n",
    "# Create sample data to demonstrate the analysis structure\n",
    "print(\"CREATING SAMPLE DATA FOR DEMONSTRATION:\")\n",
    "print(\"   (Replace this with actual dataset loading)\")\n",
    "\n",
    "# Sample data generation for demonstration\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', '2024-12-31', freq='D')\n",
    "n_days = len(dates)\n",
    "\n",
    "# Simulate temperature with seasonal pattern\n",
    "day_of_year = np.array([d.timetuple().tm_yday for d in dates])\n",
    "temp_base = 15 + 10 * np.sin(2 * np.pi * (day_of_year - 80) / 365)  # Seasonal pattern\n",
    "temp_noise = np.random.normal(0, 3, n_days)\n",
    "temperature = temp_base + temp_noise\n",
    "\n",
    "# Simulate electricity load with U-shaped temperature dependency\n",
    "comfort_temp = 20  # Comfort temperature in Celsius\n",
    "temp_effect = 0.5 * (temperature - comfort_temp) ** 2  # U-shaped relationship\n",
    "base_load = 15000  # Base load in MW\n",
    "seasonal_pattern = 1000 * np.sin(2 * np.pi * (day_of_year - 80) / 365)\n",
    "load_noise = np.random.normal(0, 200, n_days)\n",
    "electricity_load = base_load + temp_effect * 30 + seasonal_pattern + load_noise\n",
    "\n",
    "# Create demonstration dataframe\n",
    "demo_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'temperature': temperature,\n",
    "    'electricity_load': electricity_load,\n",
    "    'day_of_year': day_of_year\n",
    "})\n",
    "\n",
    "print(\"Sample data created for correlation analysis\")\n",
    "print(f\"Date range: {demo_df['date'].min()} to {demo_df['date'].max()}\")\n",
    "print(f\"üå°Ô∏è  Temperature range: {demo_df['temperature'].min():.1f}¬∞C to {demo_df['temperature'].max():.1f}¬∞C\")\n",
    "print(f\"‚ö° Load range: {demo_df['electricity_load'].min():.0f} to {demo_df['electricity_load'].max():.0f} MW\")\n",
    "\n",
    "# Correlation Analysis\n",
    "print(f\"\\nCORRELATION ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Overall correlation\n",
    "overall_corr = demo_df['temperature'].corr(demo_df['electricity_load'])\n",
    "print(f\"üìä Overall correlation (Pearson): {overall_corr:.3f}\")\n",
    "\n",
    "# Seasonal correlations\n",
    "seasons = {\n",
    "    'Winter': [12, 1, 2],\n",
    "    'Spring': [3, 4, 5], \n",
    "    'Summer': [6, 7, 8],\n",
    "    'Autumn': [9, 10, 11]\n",
    "}\n",
    "\n",
    "print(f\"\\nSEASONAL CORRELATION PATTERNS:\")\n",
    "for season, months in seasons.items():\n",
    "    season_data = demo_df[demo_df['date'].dt.month.isin(months)]\n",
    "    if len(season_data) > 0:\n",
    "        season_corr = season_data['temperature'].corr(season_data['electricity_load'])\n",
    "        avg_temp = season_data['temperature'].mean()\n",
    "        avg_load = season_data['electricity_load'].mean()\n",
    "        print(f\"   {season:8s}: r={season_corr:6.3f} | Avg Temp: {avg_temp:5.1f}¬∞C | Avg Load: {avg_load:6.0f} MW\")\n",
    "\n",
    "# Temperature ranges analysis\n",
    "print(f\"\\nüå°Ô∏è CORRELATION BY TEMPERATURE RANGES:\")\n",
    "temp_ranges = [\n",
    "    ('Cold (< 10¬∞C)', demo_df['temperature'] < 10),\n",
    "    ('Cool (10-15¬∞C)', (demo_df['temperature'] >= 10) & (demo_df['temperature'] < 15)),\n",
    "    ('Comfort (15-25¬∞C)', (demo_df['temperature'] >= 15) & (demo_df['temperature'] < 25)),\n",
    "    ('Warm (25-30¬∞C)', (demo_df['temperature'] >= 25) & (demo_df['temperature'] < 30)),\n",
    "    ('Hot (> 30¬∞C)', demo_df['temperature'] >= 30)\n",
    "]\n",
    "\n",
    "for range_name, condition in temp_ranges:\n",
    "    range_data = demo_df[condition]\n",
    "    if len(range_data) > 5:  # Need minimum data points\n",
    "        range_corr = range_data['temperature'].corr(range_data['electricity_load'])\n",
    "        count = len(range_data)\n",
    "        avg_load = range_data['electricity_load'].mean()\n",
    "        print(f\"   {range_name:15s}: r={range_corr:6.3f} | Days: {count:3d} | Avg Load: {avg_load:6.0f} MW\")\n",
    "\n",
    "print(f\"\\nKEY INSIGHTS FOR RNN/LSTM MODELING:\")\n",
    "print(\"   ‚Ä¢ U-shaped relationship: both cold and hot weather increase electricity demand\")\n",
    "print(\"   ‚Ä¢ Seasonal patterns: winter heating vs summer cooling\")\n",
    "print(\"   ‚Ä¢ Comfort zone (15-25¬∞C): minimal correlation, baseline consumption\")\n",
    "print(\"   ‚Ä¢ Temperature is a strong predictor for electricity demand\")\n",
    "print(\"   ‚Ä¢ RNN/LSTM can learn these complex non-linear relationships\")\n",
    "\n",
    "print(f\"\\nüìà WHY THIS MATTERS FOR DEEP LEARNING:\")\n",
    "print(\"   Using temperature as input feature\")\n",
    "print(\"   Explains multi-dataset approach in RNN/LSTM\")\n",
    "print(\"   Shows importance of seasonal feature engineering\")\n",
    "print(\"   Validates weather integration in forecasting API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b24268",
   "metadata": {},
   "source": [
    "## 12. Feature engineering for RNN/LSTM\n",
    "\n",
    "**Creating Multi-Dimensional input features for Deep Learning models**\n",
    "\n",
    "This section prepares the feature vectors that RNN/LSTM models will use to learn complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for RNN/LSTM Deep Learning Models\n",
    "print(\"FEATURE ENGINEERING FOR RNN/LSTM MODELS\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Creating multi-dimensional input features for deep learning\")\n",
    "print(\"This demonstrates how RNN/LSTM processes multiple datasets\")\n",
    "print()\n",
    "\n",
    "# Use the demo dataframe from previous section\n",
    "df = demo_df.copy()\n",
    "\n",
    "print(\"CREATING TEMPORAL AND SEASONAL FEATURES:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# 1. Day of year normalization (0-1 scale for neural networks)\n",
    "df['day_of_year_norm'] = df['date'].dt.dayofyear / 365.0\n",
    "print(f\"Day of year normalization: {df['day_of_year_norm'].min():.3f} - {df['day_of_year_norm'].max():.3f}\")\n",
    "\n",
    "# 2. Month encoding\n",
    "df['month'] = df['date'].dt.month\n",
    "print(f\"Month extraction: {df['month'].min()} - {df['month'].max()}\")\n",
    "\n",
    "# 3. Season one-hot encoding\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "# One-hot encode seasons\n",
    "season_dummies = pd.get_dummies(df['season'], prefix='season')\n",
    "df = pd.concat([df, season_dummies], axis=1)\n",
    "\n",
    "print(f\"Season encoding: {list(season_dummies.columns)}\")\n",
    "\n",
    "# 4. Weekend/weekday encoding\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "print(f\"Weekend encoding: {df['is_weekend'].sum()} weekend days\")\n",
    "\n",
    "# 5. Cyclical encoding for day of year (sine/cosine)\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_year_norm'])\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_year_norm'])\n",
    "print(f\"Cyclical encoding: sine and cosine transformations\")\n",
    "\n",
    "print(f\"\\nüìã FEATURE MATRIX FOR RNN/LSTM:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create the feature matrix that RNN/LSTM will use\n",
    "feature_columns = [\n",
    "    'electricity_load',    # Target variable (lagged)\n",
    "    'temperature',         # Weather predictor\n",
    "    'day_of_year_norm',   # Seasonal cycling\n",
    "    'season_Winter',      # One-hot encoded seasons\n",
    "    'season_Spring',\n",
    "    'season_Summer',\n",
    "    'season_Autumn',\n",
    "    'is_weekend',         # Weekend effect\n",
    "    'day_sin',           # Cyclical day encoding\n",
    "    'day_cos'\n",
    "]\n",
    "\n",
    "# Create feature matrix\n",
    "feature_matrix = df[feature_columns].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {feature_matrix.shape}\")\n",
    "print(f\"Number of features per time step: {len(feature_columns)}\")\n",
    "print(f\"Number of time steps (days): {len(feature_matrix)}\")\n",
    "\n",
    "print(f\"\\nFEATURE DESCRIPTION:\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"   {i:2d}. {col:20s} - {feature_matrix[col].dtype}\")\n",
    "\n",
    "# Display first few rows of feature matrix\n",
    "print(f\"\\nSAMPLE FEATURE MATRIX (First 5 days):\")\n",
    "print(\"-\" * 40)\n",
    "display(feature_matrix.head())\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nFEATURE STATISTICS:\")\n",
    "print(\"-\" * 25)\n",
    "print(feature_matrix.describe().round(3))\n",
    "\n",
    "print(f\"\\nHOW RNN/LSTM USES THESE FEATURES:\")\n",
    "print(\"-\" * 40)\n",
    "print(\" Input shape for RNN/LSTM: (batch_size, sequence_length, num_features)\")\n",
    "print(f\"   ‚Ä¢ batch_size: 32 (training batch size)\")\n",
    "print(f\"   ‚Ä¢ sequence_length: 24 (24-day lookback window)\")\n",
    "print(f\"   ‚Ä¢ num_features: {len(feature_columns)} (features per day)\")\n",
    "print(f\"   ‚Ä¢ Example tensor shape: (32, 24, {len(feature_columns)})\")\n",
    "\n",
    "print(f\"\\nSEQUENCE CREATION PROCESS:\")\n",
    "print(\"   1. Take 24 consecutive days of features\")\n",
    "print(\"   2. Use them to predict electricity load for day 25\")\n",
    "print(\"   3. Slide window by 1 day and repeat\")\n",
    "print(\"   4. Creates overlapping sequences for training\")\n",
    "\n",
    "print(f\"\\nMULTI-DATASET INTEGRATION ACHIEVED:\")\n",
    "print(\"   Electricity consumption (target variable)\")\n",
    "print(\"   Temperature data (primary predictor)\")\n",
    "print(\"   Temporal patterns (day of year, seasons)\")\n",
    "print(\"   Weekly patterns (weekend effects)\")\n",
    "print(\"   Cyclical encoding (sine/cosine)\")\n",
    "\n",
    "print(f\"\\nüìà READY FOR DEEP LEARNING:\")\n",
    "print(\"   Feature matrix prepared for RNN/LSTM training\")\n",
    "print(\"   All features normalized/encoded appropriately\")\n",
    "print(\"   Multi-dimensional time series ready\")\n",
    "print(\"   Next step: Create sliding window sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e91d69",
   "metadata": {},
   "source": [
    "## 14. Create Sequence Windows for Deep Learning\n",
    "\n",
    "**Preparing Sequential Data for RNN/LSTM Training**\n",
    "\n",
    "This is the crucial step where we transform our multi-feature time series into the sliding window format that RNN/LSTM models require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21065c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sequence Windows for RNN/LSTM Training\n",
    "print(\"CREATING SEQUENCE WINDOWS FOR RNN/LSTM\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Converting time series data into sliding window sequences\")\n",
    "print(\"This is how RNN/LSTM learns temporal patterns\")\n",
    "print()\n",
    "\n",
    "# Parameters for sequence creation\n",
    "SEQUENCE_LENGTH = 24  # 24-day lookback window\n",
    "print(f\"Sequence length: {SEQUENCE_LENGTH} days\")\n",
    "print(f\"Prediction target: Next day electricity consumption\")\n",
    "\n",
    "def create_sequences(data, target_col, sequence_length):\n",
    "    \"\"\"\n",
    "    Create sliding window sequences for RNN/LSTM training\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with features\n",
    "        target_col: Column name for target variable  \n",
    "        sequence_length: Number of days in each sequence\n",
    "    \n",
    "    Returns:\n",
    "        X: Input sequences (features)\n",
    "        y: Target values (electricity load)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize features (except target) for neural network training\n",
    "    feature_cols = [col for col in data.columns if col != target_col]\n",
    "    scaler_features = MinMaxScaler()\n",
    "    scaler_target = MinMaxScaler()\n",
    "    \n",
    "    # Fit scalers\n",
    "    data_scaled = data.copy()\n",
    "    data_scaled[feature_cols] = scaler_features.fit_transform(data[feature_cols])\n",
    "    data_scaled[[target_col]] = scaler_target.fit_transform(data[[target_col]])\n",
    "    \n",
    "    print(f\"Features normalized using MinMaxScaler\")\n",
    "    print(f\"Feature columns: {len(feature_cols)}\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(sequence_length, len(data_scaled)):\n",
    "        # Input sequence: previous 'sequence_length' days\n",
    "        sequence = data_scaled.iloc[i-sequence_length:i][data_scaled.columns].values\n",
    "        X.append(sequence)\n",
    "        \n",
    "        # Target: electricity load for current day\n",
    "        target = data_scaled.iloc[i][target_col]\n",
    "        y.append(target)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\" Sequences created:\")\n",
    "    print(f\"   ‚Ä¢ Input shape (X): {X.shape}\")\n",
    "    print(f\"   ‚Ä¢ Target shape (y): {y.shape}\")\n",
    "    print(f\"   ‚Ä¢ Interpretation:\")\n",
    "    print(f\"     - {X.shape[0]} sequences\")\n",
    "    print(f\"     - {X.shape[1]} days per sequence\")  \n",
    "    print(f\"     - {X.shape[2]} features per day\")\n",
    "    \n",
    "    return X, y, scaler_features, scaler_target\n",
    "\n",
    "# Create sequences from our feature matrix\n",
    "print(f\"\\nBUILDING SEQUENCES FROM FEATURE MATRIX:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "X_sequences, y_targets, feature_scaler, target_scaler = create_sequences(\n",
    "    feature_matrix, \n",
    "    'electricity_load', \n",
    "    SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä SEQUENCE DATASET OVERVIEW:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Original data points: {len(feature_matrix)}\")\n",
    "print(f\"Usable sequences: {len(X_sequences)}\")\n",
    "print(f\"Lost points: {len(feature_matrix) - len(X_sequences)} (first {SEQUENCE_LENGTH} days)\")\n",
    "\n",
    "print(f\"\\nEXAMPLE SEQUENCE STRUCTURE:\")\n",
    "print(\"-\" * 35)\n",
    "print(f\" Sequence 0 (first trainable sequence):\")\n",
    "print(f\"   ‚Ä¢ Input: Days {SEQUENCE_LENGTH}-{SEQUENCE_LENGTH+23} ‚Üí Day {SEQUENCE_LENGTH+24}\")\n",
    "print(f\"   ‚Ä¢ Shape: {X_sequences[0].shape}\")\n",
    "print(f\"   ‚Ä¢ Target: {y_targets[0]:.4f} (normalized)\")\n",
    "\n",
    "# Show sample of first sequence\n",
    "print(f\"\\nüëÄ SAMPLE INPUT SEQUENCE (First 3 days of sequence 0):\")\n",
    "print(\"-\" * 50)\n",
    "sample_seq = pd.DataFrame(\n",
    "    X_sequences[0][:3], \n",
    "    columns=feature_matrix.columns\n",
    ")\n",
    "display(sample_seq)\n",
    "\n",
    "# Convert to PyTorch tensors for deep learning\n",
    "X_tensor = torch.FloatTensor(X_sequences)\n",
    "y_tensor = torch.FloatTensor(y_targets)\n",
    "\n",
    "print(f\"\\nPYTORCH TENSORS CREATED:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Input tensor shape: {X_tensor.shape}\")\n",
    "print(f\"Target tensor shape: {y_tensor.shape}\")\n",
    "print(f\"Memory usage: {X_tensor.numel() * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Train/test split\n",
    "train_size = int(0.8 * len(X_sequences))\n",
    "X_train, X_test = X_tensor[:train_size], X_tensor[train_size:]\n",
    "y_train, y_test = y_tensor[:train_size], y_tensor[train_size:]\n",
    "\n",
    "print(f\"\\nTRAIN/TEST SPLIT:\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Training sequences: {len(X_train)} ({len(X_train)/len(X_sequences)*100:.1f}%)\")\n",
    "print(f\"Testing sequences: {len(X_test)} ({len(X_test)/len(X_sequences)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nREADY FOR RNN/LSTM TRAINING:\")\n",
    "print(\"-\" * 35)\n",
    "print(\"Sequences created with proper sliding windows\")\n",
    "print(\"Features normalized for neural network training\")  \n",
    "print(\"Data converted to PyTorch tensors\")\n",
    "print(\"Train/test split completed\")\n",
    "print(\"Multi-dataset integration achieved\")\n",
    "\n",
    "print(f\"\\nHOW RNN/LSTM WILL PROCESS THIS DATA:\")\n",
    "print(\"   1. Take sequence of 24 days √ó 11 features\")\n",
    "print(\"   2. Process through RNN/LSTM layers sequentially\")\n",
    "print(\"   3. Hidden states capture temporal patterns\")\n",
    "print(\"   4. Final output predicts next day electricity load\")\n",
    "print(\"   5. Backpropagate errors to learn optimal weights\")\n",
    "\n",
    "print(f\"\\nNEXT STEPS:\")\n",
    "print(\"   Define RNN/LSTM model architecture\")\n",
    "print(\"   Train models with different hyperparameters\")\n",
    "print(\"   Compare RNN vs LSTM performance\")\n",
    "print(\"   Evaluate on test set\")\n",
    "print(\"   Deploy for real-time forecasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe2905",
   "metadata": {},
   "source": [
    "## EDA summary & next steps\n",
    "\n",
    "### **Insights for RNN/LSTM models**\n",
    "\n",
    "- **Temperature-Load Correlation**: Strong U-shaped relationship justifies multi-dataset approach\n",
    "- **Seasonal Patterns**: Winter heating vs summer cooling effects captured\n",
    "- **Feature Matrix**: 11 features per time step combining electricity, weather, and temporal data\n",
    "- **Sequence Structure**: 24-day lookback windows optimal for capturing patterns\n",
    "- **Data Quality**: Clean datasets ready for deep learning\n",
    "\n",
    "### **Next**\n",
    "\n",
    "1. **Model Development**: Implement RNN and LSTM architectures\n",
    "2. **Training Pipeline**: Compare model performances and hyperparameters  \n",
    "3. **Real-time Integration**: Connect with weather API for live forecasting\n",
    "4. **API Deployment**: RESTful service for electricity demand predictions\n",
    "5. **Performance Evaluation**: Test on unseen data and validate accuracy\n",
    "\n",
    "---\n",
    "\n",
    "*Continue to `train_models.py` to build and train the RNN/LSTM models, or `api_server.py` to deploy the forecasting system.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "electricity_forecast_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
