{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c083c71",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PyTorch Input Pipeline</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69666367",
   "metadata": {},
   "source": [
    "Those who are familiar with `scikit-learn` Python package, will remember that most machine learning (ML) methods provided by the package have the same usage pattern. First, we load the training data into numpy arrays that store the features and labels of all training data points. These numpy arrays are then fed to `.fit()` function that implements the learning algorithm for a particular ML model. After the model has been trained, we apply it to the new data points, using `.predict()` function, in order to obtain predictions for the labels of new data points.\n",
    "\n",
    "In contrast, deep learning often involve extremely large training data sets which cannot be stored entirely in a numpy array (we would run out of RAM on a desktop computer). Therefore, deep learning methods use sequential access to training data. This approach ties in nicely with the working principle of stochastic gradient descent (SGD). In particular, we divide the dataset into smaller sets or parts called **batches** and only store a single batch in the working memory (e.g. as numpy arrays). The total number of samples present in a single batch is called **batch size**.\n",
    "\n",
    "After loading a new batch of data, we update the neural network parameters (weights and bias) using one iteration of an SGD variant. We repeat this batch-wise optimization until we have read each data point of the dataset. A sequence of batches that together cover the entire dataset is called an **epoch**. Note that the batch size is a hyperparameter of the resulting deep learning method. The choice of hyperparameter is often based on manual tuning (\"trial and error\") to minimize the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b91e65",
   "metadata": {},
   "source": [
    "In previous rounds we focused on deep learning models and optimizers. While model architecture and optimization algorithm are determining factors for overall performance, there is one more factor that can create a performance bottleneck: input pipeline. Opening, reading and preprocessing data consume significant amount of time and resources.\n",
    "\n",
    "The need for efficient input pipelines coincide with increased availability of GPU and TPU hardware accelerators. GPU and TPU are optimized for computations with vectors and matrices, but they are not handling data transformation and preprocessing well. Therefore, data preprocessing is often performed on CPU, which limits efficient use of GPU and TPU. Inefficient use of accelerators in its turn increases financial burden as cost of GPU and TPU use is high, not mentioning an ecological footprint from training large deep neural networks.\n",
    "\n",
    "In previous rounds we trained our model as follows:\n",
    "\n",
    "- Opening a file if it hasn't been opened yet\n",
    "- Fetching a data entry from the file\n",
    "- Using the data for training\n",
    "\n",
    "In this scenario, the process is sequential: model is sitting idle, while data is read and fetching is stall during training.\n",
    "\n",
    "**PyTorch DataLoader** is a framework for building and executing efficient input pipelines for deep learning. The idea of PyTorch input pipeline is to decouple data delivery and data consumption steps, thus decreasing time spent in idle state. This is achieved with introducing prefetching of data: while model is training on the current samples, the input pipeline prepares data samples for the next training step using multiple worker processes. This results in overlapping preprocessing with model training computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2a44a",
   "metadata": {},
   "source": [
    "**PyTorch DataLoader Features:**\n",
    "\n",
    "PyTorch DataLoader provides several key advantages for efficient data processing:\n",
    "\n",
    "- **Automatic Batching**: Seamlessly combines individual data samples into batches\n",
    "- **Shuffling**: Randomizes data order to improve training stability\n",
    "- **Parallel Data Loading**: Uses multiple worker processes to load data in parallel\n",
    "- **Memory Pinning**: Optimizes GPU transfer speeds\n",
    "- **Custom Sampling**: Supports various sampling strategies\n",
    "- **Transform Pipeline**: Integrates preprocessing and augmentation\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Building input pipelines with PyTorch\n",
    "- The basic idea of PyTorch Dataset and DataLoader APIs\n",
    "- Build custom PyTorch Dataset classes and apply various transformations\n",
    "- Optimize data loading performance with multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create tensor from list - PyTorch equivalent of tf.data.Dataset.from_tensor_slices\n",
    "tensor_data = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "print(\"Tensor:\", tensor_data)\n",
    "\n",
    "# PyTorch tensors can be iterated directly\n",
    "for value in tensor_data:\n",
    "    print(f\"Value: {value}, Numpy: {value.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24acf33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Dataset class\n",
    "class NumberDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess(x):\n",
    "    return x * x\n",
    "\n",
    "# Create dataset with preprocessing\n",
    "data = torch.arange(10).repeat(100)  # Create repeated data\n",
    "dataset = NumberDataset(data, transform=preprocess)\n",
    "\n",
    "# Create DataLoader for batching, shuffling, and parallel loading\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "print(\"Number of batches:\", len(dataloader))\n",
    "\n",
    "# Demonstrate batch processing\n",
    "for i, batch in enumerate(dataloader):\n",
    "    if i >= 3:  # Show only first 3 batches\n",
    "        break\n",
    "    print(f\"Batch {i+1}:\", batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f124f62",
   "metadata": {},
   "source": [
    "## What is a Pipeline and Why is it Needed for PyTorch?\n",
    "\n",
    "A **Pipeline** in machine learning and deep learning refers to the sequence of data processing steps that transform raw data into a format suitable for training neural networks. In PyTorch, the pipeline is crucial for several reasons:\n",
    "\n",
    "### Key Components of PyTorch Data Pipeline:\n",
    "\n",
    "1. **Dataset Class**: Abstract base class for representing a dataset\n",
    "2. **DataLoader**: Provides data batching, shuffling, and parallel loading\n",
    "3. **Transforms**: Preprocessing and augmentation operations\n",
    "4. **Samplers**: Control how data samples are drawn from the dataset\n",
    "\n",
    "### Why PyTorch Pipeline is Essential:\n",
    "\n",
    "#### 1. **Memory Efficiency**\n",
    "- **Problem**: Large datasets cannot fit entirely in memory\n",
    "- **Solution**: Load and process data in small batches on-demand\n",
    "\n",
    "#### 2. **GPU Utilization**\n",
    "- **Problem**: GPUs are fast but data loading/preprocessing is slow\n",
    "- **Solution**: Parallel data loading with multiple workers while GPU trains\n",
    "\n",
    "#### 3. **Data Preprocessing**\n",
    "- **Problem**: Raw data needs normalization, augmentation, resizing\n",
    "- **Solution**: Composable transforms that apply operations efficiently\n",
    "\n",
    "#### 4. **Batch Processing**\n",
    "- **Problem**: Neural networks work best with mini-batches\n",
    "- **Solution**: Automatic batching with customizable batch sizes\n",
    "\n",
    "#### 5. **Reproducibility**\n",
    "- **Problem**: Need consistent data ordering and randomization\n",
    "- **Solution**: Controlled shuffling and sampling with random seeds\n",
    "\n",
    "### PyTorch Pipeline vs Traditional Approach:\n",
    "\n",
    "**Traditional Approach:**\n",
    "```python\n",
    "# Load all data at once (memory intensive)\n",
    "all_images = load_all_images()\n",
    "all_labels = load_all_labels()\n",
    "\n",
    "# Process entire dataset (slow)\n",
    "processed_images = preprocess(all_images)\n",
    "\n",
    "# Manual batching\n",
    "for i in range(0, len(data), batch_size):\n",
    "    batch = processed_images[i:i+batch_size]\n",
    "    # ... training step\n",
    "```\n",
    "\n",
    "**PyTorch Pipeline Approach:**\n",
    "```python\n",
    "# Custom Dataset (lazy loading)\n",
    "dataset = CustomDataset(data_paths, transforms=my_transforms)\n",
    "\n",
    "# DataLoader (efficient batching + parallel loading)\n",
    "dataloader = DataLoader(dataset, batch_size=32, \n",
    "                       shuffle=True, num_workers=4)\n",
    "\n",
    "# Clean training loop\n",
    "for batch in dataloader:\n",
    "    # ... training step\n",
    "```\n",
    "\n",
    "### Performance Benefits:\n",
    "\n",
    "1. **Parallel Processing**: Multiple CPU cores load data while GPU trains\n",
    "2. **Prefetching**: Next batch prepared while current batch processes\n",
    "3. **Memory Management**: Only current batch in memory\n",
    "4. **Automatic Optimization**: Built-in optimizations for common patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
