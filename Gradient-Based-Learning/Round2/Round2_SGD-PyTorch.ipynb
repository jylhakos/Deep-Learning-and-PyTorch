{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46215db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n",
      "Device available: CUDA\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Adam Optimizer Examples\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Device available:\", \"CUDA\" if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7669f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.getcwd()\n",
    "#os.chdir('~/Deep-Learning-and-PyTorch/Gradient-Based-Learning/Round2')\n",
    "#import sys\n",
    "#sys.path.append(\"path\")\n",
    "#from utils import *\n",
    "# $ jupyter notebook --notebook-dir=Deep-Learning-and-PyTorch/Gradient-Based-Learning/Round2\n",
    "# PyTorch implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de39467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUDA DEVICE SETUP AND OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    device = torch.device('cuda')\n",
    "    cuda_device_name = torch.cuda.get_device_name(0)\n",
    "    cuda_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
    "    print(f\"   CUDA is available!\")\n",
    "    print(f\"   Device: {cuda_device_name}\")\n",
    "    print(f\"   Memory: {cuda_memory:.1f} GB\")\n",
    "    print(f\"   Using GPU acceleration for PyTorch computations\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"⚠️  CUDA is not available - using CPU\")\n",
    "    print(f\"   PyTorch will run on CPU (slower but still functional)\")\n",
    "\n",
    "print(f\"\\nSelected device: {device}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"WHY PYTORCH + CUDA IS OPTIMIZED FOR DEEP LEARNING\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\"\"\n",
    "TENSOR OPERATIONS ON GPU:\n",
    "• Tensors are multi-dimensional arrays perfect for parallel processing\n",
    "• GPU has thousands of cores vs CPU's few cores\n",
    "• Matrix operations (dot products, convolutions) are highly parallelizable\n",
    "• PyTorch tensors can seamlessly move between CPU and GPU\n",
    "\n",
    "⚡ CUDA ADVANTAGES:\n",
    "• Massive parallelization: 1000s of threads vs CPU's 8-16 threads  \n",
    "• Memory bandwidth: GPU memory is 10x faster than system RAM\n",
    "• Specialized cores: Tensor cores optimized for AI/ML computations\n",
    "• Automatic memory management and optimization\n",
    "\n",
    "PYTORCH CUDA OPTIMIZATIONS:\n",
    "• Automatic kernel fusion: Combines operations for efficiency\n",
    "• Memory pooling: Reduces allocation overhead\n",
    "• Mixed precision: Uses FP16 for speed, FP32 for accuracy\n",
    "• Asynchronous execution: Overlaps computation with memory transfers\n",
    "\n",
    "TYPICAL SPEEDUPS:\n",
    "• Linear algebra operations: 10-50x faster on GPU\n",
    "• Neural network training: 5-20x faster overall\n",
    "• Large batch processing: Up to 100x faster\n",
    "• Gradient computations: Highly parallelized\n",
    "\"\"\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(\"This notebook will benefit from GPU acceleration!\")\n",
    "else:\n",
    "    print(\"For GPU acceleration, ensure CUDA-compatible GPU and drivers are installed\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a7efb",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Gradient Based Optimization with PyTorch</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae4b87",
   "metadata": {},
   "source": [
    "This notebook demonstrates gradient-based optimization using **PyTorch** instead of TensorFlow/Keras. PyTorch provides automatic differentiation through its autograd system, making gradient computation much more efficient and easier to implement.\n",
    "\n",
    "We'll explore how PyTorch's built-in functions can be used for:\n",
    "- Automatic gradient computation\n",
    "- Tensor operations\n",
    "- Loss function calculations\n",
    "- Optimization algorithms\n",
    "\n",
    "The idea is to tune (adjust) the parameters according to the gradient of the average loss incurred by the neural network on a training set. This average loss is also known as the **training error** and defines an **objective or cost function** $f(\\mathbf{w})$ that we want to minimize using PyTorch's optimization tools.\n",
    "\n",
    "Here we discuss a simple iterative algorithm which is called **gradient descent** (GD) implemented with PyTorch. GD minimizes the training error by incrementally improving the current guess for the optimal parameters by moving a bit into the direction of the negative gradient. We will also discuss a slight variation of GD known as **stochastic gradient descent** (SGD). SGD is one of the most widely used optimization methods within deep learning and is readily available in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214f00e",
   "metadata": {},
   "source": [
    "### Goals\n",
    "\n",
    "- How PyTorch gradients can be used to learn the parameters of a neural network\n",
    "- The basic idea behind stochastic gradient descent (SGD) using PyTorch optimizers\n",
    "- SGD components \"batch\", \"batch size\", \"learning rate\" and \"epoch\" in PyTorch\n",
    "- PyTorch's advanced optimization algorithms such as Adam, RMSprop, and others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070621f7",
   "metadata": {},
   "source": [
    "### Recommended Resources for PyTorch\n",
    "\n",
    "- PyTorch Official Documentation: https://pytorch.org/docs/stable/index.html\n",
    "- PyTorch Tutorials: https://pytorch.org/tutorials/\n",
    "- \"Deep Learning with PyTorch\" by Eli Stevens, Luca Antiga, and Thomas Viehmann\n",
    "- PyTorch autograd documentation: https://pytorch.org/docs/stable/autograd.html\n",
    "\n",
    "### Stochastic Gradient Descent with PyTorch\n",
    "\n",
    "PyTorch provides several built-in optimizers:\n",
    "- torch.optim.SGD: Stochastic Gradient Descent\n",
    "- torch.optim.Adam: Adam optimizer\n",
    "- torch.optim.RMSprop: RMSprop optimizer\n",
    "- And many more: https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe668a0",
   "metadata": {},
   "source": [
    "PyTorch methods aim at finding a good choice for the weights (and bias) of an **artificial neural network (ANN)**. We need to define a loss function to measure how \"good\" is a particular choice for the weights. PyTorch provides many built-in loss functions in the `torch.nn` module.\n",
    "\n",
    "For a given pair of predicted label value $\\hat{y}$ and true label value $y$, PyTorch loss functions like `nn.MSELoss()` or `nn.CrossEntropyLoss()` provide a measure for the error, or \"loss\", incurred in predicting the true label $y$ by $\\hat{y}$.\n",
    "\n",
    "Some particular PyTorch loss functions that have proven useful in many applications:\n",
    "- **nn.MSELoss()**: Mean Squared Error for regression problems with numeric labels\n",
    "- **nn.CrossEntropyLoss()**: Cross Entropy Loss for classification problems\n",
    "- **nn.BCELoss()**: Binary Cross Entropy for binary classification\n",
    "- **nn.L1Loss()**: Mean Absolute Error (L1 loss)\n",
    "\n",
    "To measure the quality of particular choice for the parameters of a neural network, we use PyTorch tensors to represent our labeled data points. PyTorch's automatic differentiation system (autograd) computes gradients automatically during the **backward pass**.\n",
    "\n",
    "The training process in PyTorch typically follows this pattern:\n",
    "1. **Forward pass**: Compute predictions using the model\n",
    "2. **Loss computation**: Calculate loss using a PyTorch loss function\n",
    "3. **Backward pass**: Compute gradients using `loss.backward()`\n",
    "4. **Parameter update**: Update weights using a PyTorch optimizer\n",
    "\n",
    "By using PyTorch's built-in functions, we can solve:\n",
    "$$ \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} f(\\mathbf{w})$$\n",
    "more efficiently than manual implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01719b",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE) with PyTorch\n",
    "\n",
    "PyTorch provides the MSE loss function through `torch.nn.MSELoss()`. For numeric label values $y \\in \\mathbb{R}$, the squared error loss is:\n",
    "\n",
    "$$L(y,\\hat{y}) = (\\underbrace{y- \\hat{y}}_{\\mbox{prediction error}})^{2}.$$\n",
    "\n",
    "In PyTorch, we can compute this using:\n",
    "```python\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(predictions, targets)\n",
    "```\n",
    "\n",
    "The **mean squared error (MSE)** is computed automatically:\n",
    "$$ f(\\mathbf{w}) = (1/m) \\big( \\big( y^{(1)}-\\hat{y}^{(1)}\\big)^{2}+\\big( y^{(2)}-\\hat{y}^{(2)}\\big)^{2}+\\ldots+\\big( y^{(m)}-\\hat{y}^{(m)}\\big)^{2} \\big). $$\n",
    "\n",
    "PyTorch tensors automatically track gradients when `requires_grad=True`, making the computation much more efficient than manual implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb193d",
   "metadata": {},
   "source": [
    "The shape of the loss $f(\\mathbf{w})$, viewed as a function of the weights $\\mathbf{w}$, depends on two components. First, it depends on how the predictor map depends on the weights. Second, it depends on the choice of the loss function.\n",
    "\n",
    "PyTorch's automatic differentiation system handles the computation of gradients regardless of the complexity of the model architecture. The combination of linear operations and PyTorch's MSE loss function results in efficient gradient computation.\n",
    "\n",
    "PyTorch provides several advantages:\n",
    "- **Automatic differentiation**: No need to manually compute gradients\n",
    "- **GPU acceleration**: Seamless GPU support with `.cuda()` or `.to(device)`\n",
    "- **Built-in optimizers**: Ready-to-use optimization algorithms\n",
    "- **Dynamic computation graphs**: Flexible model architectures\n",
    "\n",
    "A convex function has the attractive property that any local minimum is always also a [global minimum](https://en.wikipedia.org/wiki/Maxima_and_minima#/media/File:Extrema_example_original.svg). PyTorch's gradient descent implementation can efficiently find these minima.\n",
    "\n",
    "<img src=\"../R2/MSELinPred.jpeg\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a619605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "MSELinPred_image = plt.imread(\"../R2/MSELinPred.jpeg\")\n",
    "plt.imshow(MSELinPred_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab8080",
   "metadata": {},
   "source": [
    "PyTorch methods use predictor maps represented by neural networks with tunable weights. In this case, the predictor depends non-linearly on the weights. As a result, we obtain (highly) non-convex loss landscapes.\n",
    "\n",
    "PyTorch's automatic differentiation system efficiently handles these complex, non-convex optimization problems. The framework provides:\n",
    "- **Automatic gradient computation** for any differentiable function\n",
    "- **Memory-efficient backpropagation** through dynamic computation graphs\n",
    "- **Advanced optimizers** that can handle non-convex landscapes better than simple SGD\n",
    "\n",
    "Below, examples of loss function landscapes of more complicated models (neural networks) illustrate that finding a minimum of these loss functions is not a trivial task, but PyTorch makes it much more manageable.\n",
    "\n",
    "<img src=\"../R2/NNloss.png\" width=500/>\n",
    "\n",
    "<center><a href=\"https://www.cs.umd.edu/~tomg/projects/landscapes/\">image source</a></center>\n",
    "<center><a href=\"https://arxiv.org/abs/1712.09913/\">original paper</a></center>\n",
    "\n",
    "Here you can find more examples of visualizations for loss functions obtained from representing a predictor map using neural networks:\n",
    "\n",
    "[3D visualization of NN loss functions](http://www.telesens.co/loss-landscape-viz/viewer.html)\n",
    "\n",
    "**Key PyTorch Advantages for Complex Loss Landscapes:**\n",
    "- Efficient computation on GPUs\n",
    "- Advanced optimizers (Adam, RMSprop, etc.) that adapt to the loss landscape\n",
    "- Automatic mixed precision for faster training\n",
    "- Easy experimentation with different architectures and loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058c5fe",
   "metadata": {},
   "source": [
    "## PyTorch Adam Optimizer\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)** is one of the most popular optimization algorithms in deep learning. PyTorch provides Adam through `torch.optim.Adam`. \n",
    "\n",
    "### Key Features of Adam:\n",
    "- **Adaptive Learning Rates**: Automatically adjusts learning rates for each parameter\n",
    "- **Momentum**: Uses moving averages of gradients (first moment)\n",
    "- **RMSprop**: Uses moving averages of squared gradients (second moment)\n",
    "- **Bias Correction**: Corrects bias in moment estimates during early training\n",
    "\n",
    "### Mathematical Foundation:\n",
    "Adam combines the best properties of AdaGrad and RMSprop:\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\quad \\text{(momentum)}$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\quad \\text{(RMSprop)}$$\n",
    "\n",
    "Where:\n",
    "- $g_t$ is the gradient at time step $t$\n",
    "- $\\beta_1$ = 0.9 (default), $\\beta_2$ = 0.999 (default)\n",
    "- $m_t$ and $v_t$ are bias-corrected first and second moment estimates\n",
    "\n",
    "### PyTorch Adam vs SGD Comparison:\n",
    "\n",
    "```python\n",
    "# SGD Optimizer\n",
    "optimizer_sgd = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Adam Optimizer (recommended for most cases)\n",
    "optimizer_adam = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ec148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Linear Model for batch training with CUDA support\n",
    "class LinearModelBatch(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LinearModelBatch, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def create_pytorch_batches(X_tensor, y_tensor, batch_size):\n",
    "    \"\"\"Create mini-batches using PyTorch DataLoader with CUDA support\"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Adam vs SGD Optimizers\n",
    "def train_with_sgd(X, y, epochs=100, lr=0.01):\n",
    "    \"\"\"Train linear model using SGD optimizer for comparison\"\"\"\n",
    "    \n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    model = LinearModel(X.shape[1])\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    losses = []\n",
    "    weights = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        predictions = model(X_tensor)\n",
    "        loss = criterion(predictions, y_tensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        weights.append(model.linear.weight.data.clone().numpy().flatten())\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "    \n",
    "    return model, losses, weights\n",
    "\n",
    "print(\"\\nTraining with SGD optimizer...\")\n",
    "model_sgd, losses_sgd, weights_sgd = train_with_sgd(X, y, epochs=100, lr=0.01)\n",
    "\n",
    "# Compare final results\n",
    "print(f\"\\nFinal Results Comparison:\")\n",
    "print(f\"Adam - Final Loss: {losses_adam[-1]:.6f}, Final Weight: {weights_adam[-1][0]:.6f}\")\n",
    "print(f\"SGD  - Final Loss: {losses_sgd[-1]:.6f}, Final Weight: {weights_sgd[-1][0]:.6f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(losses_adam, 'r-', label='Adam', linewidth=2)\n",
    "axes[0].plot(losses_sgd, 'b-', label='SGD', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Convergence: Adam vs SGD')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight evolution\n",
    "adam_weights_flat = [w[0] for w in weights_adam]\n",
    "sgd_weights_flat = [w[0] for w in weights_sgd]\n",
    "\n",
    "axes[1].plot(adam_weights_flat, 'r-', label='Adam', linewidth=2)\n",
    "axes[1].plot(sgd_weights_flat, 'b-', label='SGD', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Weight Value')\n",
    "axes[1].set_title('Weight Evolution: Adam vs SGD')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss landscape (weight vs loss)\n",
    "axes[2].plot(adam_weights_flat, losses_adam, 'ro-', label='Adam', markersize=3)\n",
    "axes[2].plot(sgd_weights_flat, losses_sgd, 'bo-', label='SGD', markersize=3)\n",
    "axes[2].set_xlabel('Weight Value')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_title('Optimization Path: Weight vs Loss')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Adam Features and Hyperparameter Tuning\n",
    "\n",
    "def compare_adam_hyperparameters(X, y, epochs=100):\n",
    "    \"\"\"Compare different Adam hyperparameter configurations\"\"\"\n",
    "    \n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    # Different Adam configurations\n",
    "    adam_configs = [\n",
    "        {'lr': 0.001, 'betas': (0.9, 0.999), 'name': 'Adam Default'},\n",
    "        {'lr': 0.01, 'betas': (0.9, 0.999), 'name': 'Adam High LR'},\n",
    "        {'lr': 0.001, 'betas': (0.5, 0.999), 'name': 'Adam Low β1'},\n",
    "        {'lr': 0.001, 'betas': (0.9, 0.99), 'name': 'Adam Low β2'},\n",
    "        {'lr': 0.001, 'betas': (0.95, 0.999), 'name': 'Adam High β1'},\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in adam_configs:\n",
    "        print(f\"\\nTraining with {config['name']}: lr={config['lr']}, betas={config['betas']}\")\n",
    "        \n",
    "        # Initialize fresh model\n",
    "        model = LinearModel(X.shape[1])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                   lr=config['lr'], \n",
    "                                   betas=config['betas'])\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            predictions = model(X_tensor)\n",
    "            loss = criterion(predictions, y_tensor)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        results[config['name']] = {\n",
    "            'losses': losses,\n",
    "            'final_loss': losses[-1],\n",
    "            'final_weight': model.linear.weight.data.item()\n",
    "        }\n",
    "        \n",
    "        print(f\"  Final Loss: {losses[-1]:.6f}, Final Weight: {model.linear.weight.data.item():.6f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run hyperparameter comparison\n",
    "print(\"=== Adam Hyperparameter Comparison ===\")\n",
    "adam_results = compare_adam_hyperparameters(X, y, epochs=100)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(2, 2, 1)\n",
    "for name, result in adam_results.items():\n",
    "    plt.plot(result['losses'], label=name, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Convergence with Different Adam Configurations')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot final losses as bar chart\n",
    "plt.subplot(2, 2, 2)\n",
    "names = list(adam_results.keys())\n",
    "final_losses = [adam_results[name]['final_loss'] for name in names]\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "bars = plt.bar(range(len(names)), final_losses, color=colors)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Final Loss')\n",
    "plt.title('Final Loss Comparison')\n",
    "plt.xticks(range(len(names)), [name.split()[1] for name in names], rotation=45)\n",
    "for bar, loss in zip(bars, final_losses):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{loss:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot weights comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "final_weights = [adam_results[name]['final_weight'] for name in names]\n",
    "bars = plt.bar(range(len(names)), final_weights, color=colors)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Final Weight')\n",
    "plt.title('Final Weight Comparison')\n",
    "plt.xticks(range(len(names)), [name.split()[1] for name in names], rotation=45)\n",
    "for bar, weight in zip(bars, final_weights):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{weight:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Learning rate sensitivity\n",
    "plt.subplot(2, 2, 4)\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "lr_losses = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = LinearModel(X.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    for _ in range(50):  # Quick training\n",
    "        predictions = model(X_tensor)\n",
    "        loss = criterion(predictions, y_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    lr_losses.append(loss.item())\n",
    "\n",
    "plt.semilogx(learning_rates, lr_losses, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Final Loss (50 epochs)')\n",
    "plt.title('Adam Learning Rate Sensitivity')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8e25d",
   "metadata": {},
   "source": [
    "## Adam Optimizer: Best Practices & Recommendations\n",
    "\n",
    "### When to Use Adam vs SGD\n",
    "\n",
    "**Use Adam when:**\n",
    "-  **Quick prototyping**: Adam often works well with default parameters\n",
    "-  **Sparse gradients**: Better handling of sparse features\n",
    "-  **Non-stationary objectives**: When the optimization landscape changes\n",
    "-  **Fast initial progress**: Adam typically converges faster initially\n",
    "\n",
    "**Use SGD when:**\n",
    "-  **Final performance**: SGD often achieves better generalization\n",
    "-  **Memory constraints**: SGD uses less memory (no momentum storage)\n",
    "-  **Fine-tuning**: When you need precise control over learning dynamics\n",
    "-  **Well-studied problems**: When you know good SGD hyperparameters\n",
    "\n",
    "### Adam Hyperparameter Guidelines\n",
    "\n",
    "| Parameter | Default | Range | Purpose |\n",
    "|-----------|---------|-------|---------|\n",
    "| `lr` (learning rate) | 0.001 | 1e-5 to 1e-1 | Controls step size |\n",
    "| `beta1` (β₁) | 0.9 | 0.8 to 0.95 | Momentum decay rate |\n",
    "| `beta2` (β₂) | 0.999 | 0.99 to 0.9999 | RMSprop decay rate |\n",
    "| `eps` (ε) | 1e-8 | 1e-10 to 1e-6 | Numerical stability |\n",
    "| `weight_decay` | 0 | 1e-6 to 1e-2 | L2 regularization |\n",
    "\n",
    "### Common PyTorch Adam Patterns\n",
    "\n",
    "```python\n",
    "# Standard Adam setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Adam with weight decay (AdamW alternative)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Adam with learning rate scheduling\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Training loop with Adam\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(model(batch.x), batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()  # Update learning rate\n",
    "```\n",
    "\n",
    "### Adam vs Other Optimizers Summary\n",
    "\n",
    "| Optimizer | Convergence Speed | Final Performance | Memory Usage | Stability |\n",
    "|-----------|------------------|-------------------|--------------|-----------|\n",
    "| **Adam** | HIGH | MEDIUM | LOW | HIGH |\n",
    "| **SGD** | LOW | HIGH | HIGH | MEDIUM |\n",
    "| **AdamW** | HIGH | HIGH | LOW | HIGH |\n",
    "| **RMSprop** | MEDIUM | MEDIUM | MEDIUM | MEDIUM |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a7ef8",
   "metadata": {},
   "source": [
    "# Batch vs Mini-Batch Training in PyTorch\n",
    "\n",
    "## Understanding Different Batch Training Approaches\n",
    "\n",
    "This section demonstrates the fundamental differences between:\n",
    "- **Full Batch Training**: Uses entire dataset for each gradient update\n",
    "- **Mini-Batch Training**: Uses small subsets of data for each gradient update  \n",
    "- **Stochastic Gradient Descent (SGD)**: Uses single samples for each gradient update\n",
    "\n",
    "Each approach has different trade-offs in terms of:\n",
    "- Memory usage\n",
    "- Convergence speed\n",
    "- Computational efficiency\n",
    "- Final model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec53d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_full_batch(X, y, epochs=50, lr=0.01):\n",
    "    \"\"\"Full batch training in PyTorch with CUDA support\"\"\"\n",
    "    print(f\"PyTorch FULL BATCH training (dataset size: {len(X)})\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Convert to tensors and move to device (GPU/CPU)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    \n",
    "    # Initialize model and move to device\n",
    "    model = LinearModelBatch(X.shape[1]).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training\n",
    "    losses = []\n",
    "    weights = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Full batch forward pass\n",
    "        predictions = model(X_tensor)\n",
    "        loss = criterion(predictions, y_tensor)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Store metrics (move to CPU for numpy conversion)\n",
    "        losses.append(loss.item())\n",
    "        weights.append(model.linear.weight.data.cpu().clone().numpy().flatten()[0])\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"  Epoch {epoch:3d}/{epochs}, Loss: {loss.item():.6f}, Weight: {weights[-1]:.6f}\")\n",
    "    \n",
    "    return model, losses, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_mini_batch(X, y, epochs=50, lr=0.01, batch_size=10):\n",
    "    \"\"\"Mini-batch training in PyTorch with CUDA support\"\"\"\n",
    "    print(f\"PyTorch MINI-BATCH training (batch size: {batch_size})\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Convert to tensors and move to device (GPU/CPU)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    \n",
    "    # Initialize model and move to device\n",
    "    model = LinearModelBatch(X.shape[1]).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Create data loader\n",
    "    dataloader = create_pytorch_batches(X_tensor, y_tensor, batch_size)\n",
    "    \n",
    "    # Training\n",
    "    losses = []\n",
    "    weights = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for batch_X, batch_y in dataloader:\n",
    "            # Move batch to device if needed (DataLoader might not preserve device)\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Mini-batch forward pass\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "        weights.append(model.linear.weight.data.cpu().clone().numpy().flatten()[0])\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"  Epoch {epoch:3d}/{epochs}, Loss: {avg_loss:.6f}, Weight: {weights[-1]:.6f}\")\n",
    "    \n",
    "    return model, losses, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset for PyTorch batch comparison\n",
    "print(\"Generating dataset for PyTorch batch comparison...\")\n",
    "np.random.seed(42)\n",
    "X_pytorch, y_pytorch = make_regression(n_samples=500, n_features=3, noise=15, random_state=42)\n",
    "X_pytorch = StandardScaler().fit_transform(X_pytorch)  # Normalize features\n",
    "y_pytorch = StandardScaler().fit_transform(y_pytorch.reshape(-1, 1)).flatten()  # Normalize target\n",
    "\n",
    "print(f\"PyTorch dataset shape: X={X_pytorch.shape}, y={y_pytorch.shape}\")\n",
    "\n",
    "# Run PyTorch batch comparisons\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PYTORCH BATCH TRAINING COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. PyTorch FULL BATCH:\")\n",
    "model_full, losses_full, weights_full, time_full = train_pytorch_full_batch(\n",
    "    X_pytorch, y_pytorch, epochs=40, lr=0.01)\n",
    "\n",
    "print(\"\\n2. PyTorch MINI-BATCH (batch_size=25):\")\n",
    "model_mini25, losses_mini25, weights_mini25, time_mini25 = train_pytorch_mini_batch(\n",
    "    X_pytorch, y_pytorch, epochs=40, lr=0.01, batch_size=25)\n",
    "\n",
    "print(\"\\n3. PyTorch MINI-BATCH (batch_size=50):\")\n",
    "model_mini50, losses_mini50, weights_mini50, time_mini50 = train_pytorch_mini_batch(\n",
    "    X_pytorch, y_pytorch, epochs=40, lr=0.01, batch_size=50)\n",
    "\n",
    "print(\"\\n4. PyTorch SGD (batch_size=1):\")\n",
    "model_sgd, losses_sgd, weights_sgd, time_sgd = train_pytorch_sgd(\n",
    "    X_pytorch, y_pytorch, epochs=20, lr=0.01)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Full Batch (500) - Final Loss: {losses_full[-1]:.6f}, Time: {time_full:.2f}s\")\n",
    "print(f\"Mini-Batch (25)  - Final Loss: {losses_mini25[-1]:.6f}, Time: {time_mini25:.2f}s\")\n",
    "print(f\"Mini-Batch (50)  - Final Loss: {losses_mini50[-1]:.6f}, Time: {time_mini50:.2f}s\")\n",
    "print(f\"SGD (1)          - Final Loss: {losses_sgd[-1]:.6f}, Time: {time_sgd:.2f}s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive PyTorch batch analysis visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('PyTorch: Batch vs Mini-Batch Training Analysis', fontsize=16)\n",
    "\n",
    "# Loss convergence comparison\n",
    "axes[0, 0].plot(losses_full, 'b-', linewidth=2, label='Full Batch (500)')\n",
    "axes[0, 0].plot(losses_mini25, 'r-', linewidth=2, label='Mini-Batch (25)')\n",
    "axes[0, 0].plot(losses_mini50, 'g-', linewidth=2, label='Mini-Batch (50)')\n",
    "axes[0, 0].plot(losses_sgd, 'm-', linewidth=2, label='SGD (1)', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Loss Convergence Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight evolution (first weight only for multi-feature model)\n",
    "axes[0, 1].plot(weights_full, 'b-', linewidth=2, label='Full Batch')\n",
    "axes[0, 1].plot(weights_mini25, 'r-', linewidth=2, label='Mini-Batch (25)')\n",
    "axes[0, 1].plot(weights_mini50, 'g-', linewidth=2, label='Mini-Batch (50)')\n",
    "axes[0, 1].plot(weights_sgd, 'm-', linewidth=2, label='SGD (1)', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('First Weight Value')\n",
    "axes[0, 1].set_title('Weight Evolution Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "methods = ['Full Batch\\n(500)', 'Mini-Batch\\n(25)', 'Mini-Batch\\n(50)', 'SGD\\n(1)']\n",
    "times = [time_full, time_mini25, time_mini50, time_sgd]\n",
    "colors = ['blue', 'red', 'green', 'magenta']\n",
    "\n",
    "bars = axes[0, 2].bar(methods, times, color=colors, alpha=0.7)\n",
    "axes[0, 2].set_ylabel('Training Time (seconds)')\n",
    "axes[0, 2].set_title('Training Time Comparison')\n",
    "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time_val in zip(bars, times):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "# Memory usage analysis (theoretical)\n",
    "batch_sizes_analysis = [1, 25, 50, 100, 250, len(X_pytorch)]\n",
    "memory_usage = [size * X_pytorch.shape[1] * 4 / 1024 for size in batch_sizes_analysis]  # KB\n",
    "\n",
    "axes[1, 0].semilogx(batch_sizes_analysis, memory_usage, 'o-', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Batch Size (log scale)')\n",
    "axes[1, 0].set_ylabel('Memory Usage (KB)')\n",
    "axes[1, 0].set_title('Memory Usage vs Batch Size')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Updates per epoch analysis\n",
    "batch_sizes_for_updates = [1, 25, 50, 500]  # Our actual batch sizes\n",
    "updates_per_epoch = [len(X_pytorch) // size for size in batch_sizes_for_updates]\n",
    "\n",
    "axes[1, 1].bar(range(len(batch_sizes_for_updates)), updates_per_epoch, \n",
    "               color=['magenta', 'red', 'green', 'blue'], alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Batch Configuration')\n",
    "axes[1, 1].set_ylabel('Gradient Updates per Epoch')\n",
    "axes[1, 1].set_title('Gradient Updates per Epoch')\n",
    "axes[1, 1].set_xticks(range(len(batch_sizes_for_updates)))\n",
    "axes[1, 1].set_xticklabels([f'Batch={size}' for size in batch_sizes_for_updates])\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, updates in enumerate(updates_per_epoch):\n",
    "    axes[1, 1].text(i, updates + 1, str(updates), ha='center', va='bottom')\n",
    "\n",
    "# Final performance comparison\n",
    "final_losses = [losses_full[-1], losses_mini25[-1], losses_mini50[-1], losses_sgd[-1]]\n",
    "bars = axes[1, 2].bar(methods, final_losses, color=colors, alpha=0.7)\n",
    "axes[1, 2].set_ylabel('Final Loss')\n",
    "axes[1, 2].set_title('Final Performance Comparison')\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, loss in zip(bars, final_losses):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{loss:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"\\nDETAILED ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"• Full Batch processes {len(X_pytorch)} samples per update\")\n",
    "print(f\"• Mini-Batch (25) processes 25 samples per update, {len(X_pytorch)//25} updates/epoch\")\n",
    "print(f\"• Mini-Batch (50) processes 50 samples per update, {len(X_pytorch)//50} updates/epoch\")\n",
    "print(f\"• SGD processes 1 sample per update, {len(X_pytorch)} updates/epoch\")\n",
    "print(f\"\\n• Memory usage scales linearly with batch size\")\n",
    "print(f\"• Mini-batches provide good balance of speed and stability\")\n",
    "print(f\"• SGD is noisiest but can escape local minima\")\n",
    "print(f\"• Full batch has smoothest convergence but slowest per-epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ebe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced PyTorch DataLoader Features\n",
    "print(\"=\" * 60)\n",
    "print(\"ADVANCED PYTORCH DATALOADER FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def demonstrate_dataloader_features():\n",
    "    \"\"\"Demonstrate advanced DataLoader capabilities\"\"\"\n",
    "    \n",
    "    # Create a sample dataset\n",
    "    X_demo = torch.randn(100, 5)  # 100 samples, 5 features\n",
    "    y_demo = torch.randn(100, 1)  # 100 targets\n",
    "    \n",
    "    print(\"\\n1. BASIC DATALOADER:\")\n",
    "    dataset = torch.utils.data.TensorDataset(X_demo, y_demo)\n",
    "    basic_loader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "    \n",
    "    print(f\"   Dataset size: {len(dataset)}\")\n",
    "    print(f\"   Batch size: {basic_loader.batch_size}\")\n",
    "    print(f\"   Number of batches: {len(basic_loader)}\")\n",
    "    \n",
    "    # Show first batch\n",
    "    for i, (batch_x, batch_y) in enumerate(basic_loader):\n",
    "        print(f\"   Batch {i+1}: X shape {batch_x.shape}, y shape {batch_y.shape}\")\n",
    "        if i == 0:  # Only show first batch\n",
    "            break\n",
    "    \n",
    "    print(\"\\n2. DATALOADER WITH DROP_LAST:\")\n",
    "    drop_last_loader = torch.utils.data.DataLoader(dataset, batch_size=30, \n",
    "                                                   shuffle=True, drop_last=True)\n",
    "    print(f\"   With drop_last=True: {len(drop_last_loader)} batches\")\n",
    "    \n",
    "    drop_last_false_loader = torch.utils.data.DataLoader(dataset, batch_size=30, \n",
    "                                                         shuffle=True, drop_last=False)\n",
    "    print(f\"   With drop_last=False: {len(drop_last_false_loader)} batches\")\n",
    "    \n",
    "    print(\"\\n3. BATCH SIZE EFFECTS:\")\n",
    "    batch_sizes = [10, 25, 50, 100]\n",
    "    for bs in batch_sizes:\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=bs)\n",
    "        print(f\"   Batch size {bs:3d}: {len(loader)} batches\")\n",
    "    \n",
    "    print(\"\\n4. SHUFFLE COMPARISON:\")\n",
    "    # Without shuffle\n",
    "    no_shuffle_loader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=False)\n",
    "    batch1_no_shuffle = next(iter(no_shuffle_loader))[0][0]  # First sample of first batch\n",
    "    \n",
    "    # With shuffle\n",
    "    shuffle_loader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "    batch1_shuffle = next(iter(shuffle_loader))[0][0]  # First sample of first batch\n",
    "    \n",
    "    print(f\"   First sample without shuffle: {batch1_no_shuffle[:3].numpy()}\")\n",
    "    print(f\"   First sample with shuffle: {batch1_shuffle[:3].numpy()}\")\n",
    "    print(\"   (Note: Values will be different due to shuffling)\")\n",
    "\n",
    "demonstrate_dataloader_features()\n",
    "\n",
    "# Demonstrate the importance of batch size selection\n",
    "print(\"\\n5. BATCH SIZE SELECTION IMPACT:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def test_batch_size_impact():\n",
    "    \"\"\"Test different batch sizes on the same problem\"\"\"\n",
    "    X_test = torch.randn(200, 4)\n",
    "    y_test = torch.sum(X_test, dim=1, keepdim=True) + 0.1 * torch.randn(200, 1)\n",
    "    \n",
    "    batch_sizes = [1, 10, 50, 200]  # SGD, small mini-batch, medium mini-batch, full batch\n",
    "    results = {}\n",
    "    \n",
    "    for bs in batch_sizes:\n",
    "        model = LinearModelBatch(4)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(20):\n",
    "            epoch_loss = 0\n",
    "            num_batches = 0\n",
    "            for batch_x, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(batch_x)\n",
    "                loss = criterion(pred, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            losses.append(avg_loss)\n",
    "        \n",
    "        results[f'Batch_{bs}'] = {\n",
    "            'final_loss': losses[-1],\n",
    "            'losses': losses,\n",
    "            'batch_size': bs\n",
    "        }\n",
    "        \n",
    "        batch_type = 'SGD' if bs == 1 else 'Full Batch' if bs == 200 else 'Mini-Batch'\n",
    "        print(f\"   {batch_type} (size={bs}): Final Loss = {losses[-1]:.6f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "batch_impact_results = test_batch_size_impact()\n",
    "\n",
    "# Visualize batch size impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot loss curves\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "for i, (name, result) in enumerate(batch_impact_results.items()):\n",
    "    label = f\"Batch Size {result['batch_size']}\"\n",
    "    ax1.plot(result['losses'], color=colors[i], linewidth=2, label=label)\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Convergence for Different Batch Sizes')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart of final losses\n",
    "batch_sizes = [result['batch_size'] for result in batch_impact_results.values()]\n",
    "final_losses = [result['final_loss'] for result in batch_impact_results.values()]\n",
    "\n",
    "bars = ax2.bar([f'Size {bs}' for bs in batch_sizes], final_losses, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Final Loss')\n",
    "ax2.set_title('Final Loss by Batch Size')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, loss in zip(bars, final_losses):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{loss:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBatch size impact analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa2c3b",
   "metadata": {},
   "source": [
    "## PyTorch Batch vs Mini-Batch: Key Takeaways\n",
    "\n",
    "### **When to use Each approach?**\n",
    "\n",
    "#### Full batch Trainint\n",
    "- **Best for**: Small datasets (< 1000 samples)\n",
    "- **Advantages**: Smoothest convergence, most accurate gradients\n",
    "- **Disadvantages**: Memory intensive, slower per epoch\n",
    "- **Use cases**: Final fine-tuning, small-scale experiments\n",
    "\n",
    "#### Mini-Batch training  \n",
    "- **Best for**: Most deep learning applications\n",
    "- **Advantages**: Memory efficient, good balance of speed/stability\n",
    "- **Disadvantages**: Slightly noisy gradients\n",
    "- **Recommended sizes**: 32-128 for most problems\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "- **Best for**: Very large datasets, online learning\n",
    "- **Advantages**: Minimal memory usage, can escape local minima\n",
    "- **Disadvantages**: Very noisy convergence, poor GPU utilization\n",
    "- **Use cases**: Streaming data, extremely memory-constrained environments\n",
    "\n",
    "### **PyTorch advantages for batch processing**\n",
    "\n",
    "1. **DataLoader**: Automatic batching, shuffling, and parallel loading\n",
    "2. **GPU Acceleration**: Efficient parallel processing within batches  \n",
    "3. **Memory Management**: Automatic gradient accumulation and cleanup\n",
    "4. **Flexibility**: Easy to experiment with different batch sizes\n",
    "5. **Integration**: Seamless with neural network architectures\n",
    "\n",
    "### **Recommendations**\n",
    "\n",
    "- **Start with batch_size=32** for most problems\n",
    "- **Increase batch size** if you have more GPU memory\n",
    "- **Use power-of-2 batch sizes** (16, 32, 64, 128) for GPU efficiency\n",
    "- **Enable shuffling** for training, disable for validation\n",
    "- **Monitor memory usage** and adjust batch size accordingly\n",
    "- **Consider gradient accumulation** for very large effective batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc10975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GPU vs CPU PERFORMANCE COMPARISON (if CUDA is available)\n",
    "# =============================================================================\n",
    "\n",
    "if cuda_available:\n",
    "    print(\"DEMONSTRATING GPU vs CPU PERFORMANCE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Create a larger dataset for performance comparison\n",
    "    X_large, y_large = make_regression(n_samples=5000, n_features=50, noise=10, random_state=42)\n",
    "    \n",
    "    def benchmark_device_performance(X, y, device_name, device_obj, epochs=10):\n",
    "        \"\"\"Benchmark training performance on specified device\"\"\"\n",
    "        print(f\"\\n📊 Training on {device_name}...\")\n",
    "        \n",
    "        # Move data to device\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device_obj)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1).to(device_obj)\n",
    "        \n",
    "        # Create model\n",
    "        model = LinearModelBatch(X.shape[1]).to(device_obj)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Warm-up (important for GPU)\n",
    "        for _ in range(3):\n",
    "            predictions = model(X_tensor)\n",
    "            loss = criterion(predictions, y_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            predictions = model(X_tensor)\n",
    "            loss = criterion(predictions, y_tensor)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if device_obj.type == 'cuda':\n",
    "                torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_time = end_time - start_time\n",
    "        print(f\"   Time: {training_time:.3f} seconds\")\n",
    "        print(f\"   Final Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        return training_time, loss.item()\n",
    "    \n",
    "    # Benchmark CPU\n",
    "    cpu_time, cpu_loss = benchmark_device_performance(\n",
    "        X_large, y_large, \"CPU\", torch.device('cpu'), epochs=20\n",
    "    )\n",
    "    \n",
    "    # Benchmark GPU\n",
    "    gpu_time, gpu_loss = benchmark_device_performance(\n",
    "        X_large, y_large, \"GPU (CUDA)\", torch.device('cuda'), epochs=20\n",
    "    )\n",
    "    \n",
    "    # Results\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nPERFORMANCE RESULTS:\")\n",
    "    print(f\"   CPU Time: {cpu_time:.3f}s\")\n",
    "    print(f\"   GPU Time: {gpu_time:.3f}s\")\n",
    "    print(f\"   Speedup: {speedup:.2f}x faster on GPU\")\n",
    "    \n",
    "    if speedup > 2:\n",
    "        print(\"   Significant GPU acceleration achieved!\")\n",
    "    elif speedup > 1.2:\n",
    "        print(\"   ⚡ Moderate GPU acceleration achieved!\")\n",
    "    else:\n",
    "        print(\"   ⚠️  Limited acceleration - dataset might be too small for GPU benefits\")\n",
    "    \n",
    "    print(f\"\\n Insights:\")\n",
    "    print(f\"   • GPU excels at parallel matrix operations\")\n",
    "    print(f\"   • Larger datasets see bigger GPU benefits\")\n",
    "    print(f\"   • Memory transfer overhead affects small datasets\")\n",
    "    print(f\"   • Tensor cores accelerate FP16 operations\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ CUDA not available - GPU performance comparison skipped\")\n",
    "    print(\"   To enable GPU acceleration:\")\n",
    "    print(\"   1. Install CUDA-compatible GPU\")\n",
    "    print(\"   2. Install CUDA drivers and toolkit\")\n",
    "    print(\"   3. Install PyTorch with CUDA support:\")\n",
    "    print(\"      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
