{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450ce53e",
   "metadata": {},
   "source": [
    "# Batch vs Mini-Batch Training in Deep Learning\n",
    "## TensorFlow/Keras Implementation\n",
    "\n",
    "This notebook demonstrates the differences between batch and mini-batch training approaches using TensorFlow/Keras, NumPy, and Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adfc09e",
   "metadata": {},
   "source": [
    "## Theory: Understanding Batch vs Mini-Batch Training\n",
    "\n",
    "### Batch Training (Batch Gradient Descent)\n",
    "- Uses the **entire training dataset** for each gradient update\n",
    "- More accurate gradient estimation\n",
    "- Smoother convergence path\n",
    "- Memory intensive for large datasets\n",
    "- Slower per-epoch training\n",
    "- Better for smaller datasets\n",
    "\n",
    "### Mini-Batch Training (Mini-Batch Gradient Descent)\n",
    "- Uses **small subsets** of the training data for each gradient update\n",
    "- Less accurate gradient estimation but faster training\n",
    "- More noisy convergence path but often finds better minima\n",
    "- Memory efficient for large datasets\n",
    "- Faster per-epoch training\n",
    "- **Industry standard** for deep learning\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "- Uses **single sample** for each gradient update\n",
    "- Fastest per-update but most noisy\n",
    "- Can escape local minima due to noise\n",
    "- Most memory efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import TensorFlow - handle case where it's not installed\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    TF_AVAILABLE = True\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"Keras version: {keras.__version__}\")\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow not available. Will demonstrate with NumPy and Pandas only.\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96c062",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# Regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=10000, n_features=20, noise=0.1, random_state=42)\n",
    "X_reg = StandardScaler().fit_transform(X_reg)\n",
    "\n",
    "# Classification dataset  \n",
    "X_clf, y_clf = make_classification(n_samples=10000, n_features=20, n_classes=2, \n",
    "                                   n_redundant=0, random_state=42)\n",
    "X_clf = StandardScaler().fit_transform(X_clf)\n",
    "\n",
    "print(f\"Regression dataset shape: X={X_reg.shape}, y={y_reg.shape}\")\n",
    "print(f\"Classification dataset shape: X={X_clf.shape}, y={y_clf.shape}\")\n",
    "\n",
    "# Split datasets\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining sets:\")\n",
    "print(f\"Regression: X_train={X_reg_train.shape}, y_train={y_reg_train.shape}\")\n",
    "print(f\"Classification: X_train={X_clf_train.shape}, y_train={y_clf_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e45b91",
   "metadata": {},
   "source": [
    "## 2. NumPy Implementation: Batch vs Mini-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNumPy:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.history = {'loss': [], 'batch_sizes': []}\n",
    "    \n",
    "    def initialize_parameters(self, n_features):\n",
    "        \"\"\"Initialize weights and bias\"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.weights = np.random.normal(0, 0.01, n_features)\n",
    "        self.bias = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        return X.dot(self.weights) + self.bias\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute MSE loss\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def compute_gradients(self, X, y_true, y_pred):\n",
    "        \"\"\"Compute gradients\"\"\"\n",
    "        m = X.shape[0]\n",
    "        dw = (-2/m) * X.T.dot(y_true - y_pred)\n",
    "        db = (-2/m) * np.sum(y_true - y_pred)\n",
    "        return dw, db\n",
    "    \n",
    "    def update_parameters(self, dw, db):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "        self.weights -= self.learning_rate * dw\n",
    "        self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def train_batch(self, X, y, epochs=100):\n",
    "        \"\"\"Full batch training\"\"\"\n",
    "        print(\"Training with FULL BATCH (entire dataset)...\")\n",
    "        self.initialize_parameters(X.shape[1])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass on entire dataset\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            \n",
    "            # Backward pass on entire dataset\n",
    "            dw, db = self.compute_gradients(X, y, y_pred)\n",
    "            self.update_parameters(dw, db)\n",
    "            \n",
    "            self.history['loss'].append(loss)\n",
    "            self.history['batch_sizes'].append(len(X))\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"  Epoch {epoch:3d}/{epochs}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"  Training completed in {training_time:.2f} seconds\")\n",
    "        return self.history\n",
    "    \n",
    "    def train_mini_batch(self, X, y, epochs=100, batch_size=32):\n",
    "        \"\"\"Mini-batch training\"\"\"\n",
    "        print(f\"Training with MINI-BATCH (batch_size={batch_size})...\")\n",
    "        self.initialize_parameters(X.shape[1])\n",
    "        self.history = {'loss': [], 'batch_sizes': []}\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data for each epoch\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_losses = []\n",
    "            \n",
    "            for batch_idx in range(n_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass on mini-batch\n",
    "                y_pred = self.forward(X_batch)\n",
    "                loss = self.compute_loss(y_batch, y_pred)\n",
    "                \n",
    "                # Backward pass on mini-batch\n",
    "                dw, db = self.compute_gradients(X_batch, y_batch, y_pred)\n",
    "                self.update_parameters(dw, db)\n",
    "                \n",
    "                epoch_losses.append(loss)\n",
    "                self.history['batch_sizes'].append(batch_size)\n",
    "            \n",
    "            # Average loss for the epoch\n",
    "            avg_epoch_loss = np.mean(epoch_losses)\n",
    "            self.history['loss'].append(avg_epoch_loss)\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"  Epoch {epoch:3d}/{epochs}, Loss: {avg_epoch_loss:.6f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"  Training completed in {training_time:.2f} seconds\")\n",
    "        return self.history\n",
    "\n",
    "print(\"NumPy implementation ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NumPy comparison\n",
    "print(\"NumPy Implementation Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Use smaller dataset for faster training\n",
    "X_small = X_reg_train[:1000]\n",
    "y_small = y_reg_train[:1000]\n",
    "\n",
    "# Full Batch\n",
    "model_batch = LinearRegressionNumPy(learning_rate=0.01)\n",
    "history_batch = model_batch.train_batch(X_small, y_small, epochs=50)\n",
    "\n",
    "# Mini-Batch\n",
    "model_mini = LinearRegressionNumPy(learning_rate=0.01)\n",
    "history_mini = model_mini.train_mini_batch(X_small, y_small, epochs=50, batch_size=64)\n",
    "\n",
    "# SGD (batch_size=1)\n",
    "model_sgd = LinearRegressionNumPy(learning_rate=0.01)\n",
    "history_sgd = model_sgd.train_mini_batch(X_small, y_small, epochs=20, batch_size=1)  # Fewer epochs for SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc33358",
   "metadata": {},
   "source": [
    "## 3. Pandas Implementation: Creating Mini-Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de821c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches_pandas(df, batch_size):\n",
    "    \"\"\"Create mini-batches from pandas DataFrame\"\"\"\n",
    "    # Shuffle the dataframe\n",
    "    df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for i in range(0, len(df_shuffled), batch_size):\n",
    "        batch = df_shuffled.iloc[i:i+batch_size]\n",
    "        batches.append(batch)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns = [f'feature_{i}' for i in range(X_small.shape[1])]\n",
    "df_reg = pd.DataFrame(X_small, columns=columns)\n",
    "df_reg['target'] = y_small\n",
    "\n",
    "print(f\"DataFrame shape: {df_reg.shape}\")\n",
    "print(\"\\nDataFrame head:\")\n",
    "print(df_reg.head())\n",
    "\n",
    "# Create different batch sizes\n",
    "batch_sizes = [32, 128, 512, len(df_reg)]\n",
    "batch_examples = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if batch_size >= len(df_reg):\n",
    "        batch_name = \"Full Batch\"\n",
    "        batches = [df_reg]\n",
    "    else:\n",
    "        batch_name = f\"Mini-Batch (size={batch_size})\"\n",
    "        batches = create_mini_batches_pandas(df_reg, batch_size)\n",
    "    \n",
    "    batch_examples[batch_name] = {\n",
    "        'batch_size': batch_size,\n",
    "        'num_batches': len(batches),\n",
    "        'batches': batches[:2]  # Store first 2 batches as examples\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{batch_name}:\")\n",
    "    print(f\"  Number of batches: {len(batches)}\")\n",
    "    print(f\"  Batch size: {batch_size if batch_size < len(df_reg) else len(df_reg)}\")\n",
    "    print(f\"  First batch shape: {batches[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99d778",
   "metadata": {},
   "source": [
    "## 4. TensorFlow/Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    def create_keras_model(input_dim, task='regression'):\n",
    "        \"\"\"Create a simple Keras model\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(16, activation='relu'),\n",
    "            layers.Dense(1, activation='sigmoid' if task == 'classification' else None)\n",
    "        ])\n",
    "        \n",
    "        if task == 'regression':\n",
    "            model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        else:\n",
    "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train_with_different_batch_sizes(X_train, y_train, X_test, y_test, task='regression'):\n",
    "        \"\"\"Train models with different batch sizes\"\"\"\n",
    "        batch_sizes = [16, 64, 256, len(X_train)]  # Include full batch\n",
    "        results = {}\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            batch_name = \"Full Batch\" if batch_size >= len(X_train) else f\"Batch Size {batch_size}\"\n",
    "            print(f\"\\nTraining with {batch_name}...\")\n",
    "            \n",
    "            # Create fresh model\n",
    "            model = create_keras_model(X_train.shape[1], task)\n",
    "            \n",
    "            # Adjust batch size for full batch\n",
    "            actual_batch_size = min(batch_size, len(X_train))\n",
    "            \n",
    "            start_time = time.time()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                batch_size=actual_batch_size,\n",
    "                epochs=20,\n",
    "                validation_data=(X_test, y_test),\n",
    "                verbose=0\n",
    "            )\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            results[batch_name] = {\n",
    "                'batch_size': actual_batch_size,\n",
    "                'history': history,\n",
    "                'training_time': training_time,\n",
    "                'final_loss': history.history['loss'][-1],\n",
    "                'final_val_loss': history.history['val_loss'][-1]\n",
    "            }\n",
    "            \n",
    "            print(f\"  Training time: {training_time:.2f}s\")\n",
    "            print(f\"  Final loss: {history.history['loss'][-1]:.4f}\")\n",
    "            print(f\"  Final val loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    # Train models if TensorFlow is available\n",
    "    print(\"TensorFlow/Keras Implementation:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Use smaller dataset for faster training\n",
    "    X_reg_small = X_reg_train[:2000]\n",
    "    y_reg_small = y_reg_train[:2000]\n",
    "    X_reg_test_small = X_reg_test[:400]\n",
    "    y_reg_test_small = y_reg_test[:400]\n",
    "    \n",
    "    print(\"Training REGRESSION models with different batch sizes:\")\n",
    "    regression_results = train_with_different_batch_sizes(\n",
    "        X_reg_small, y_reg_small, X_reg_test_small, y_reg_test_small, 'regression')\n",
    "    \n",
    "    print(\"\\nTraining CLASSIFICATION models with different batch sizes:\")\n",
    "    X_clf_small = X_clf_train[:2000]\n",
    "    y_clf_small = y_clf_train[:2000]\n",
    "    X_clf_test_small = X_clf_test[:400]\n",
    "    y_clf_test_small = y_clf_test[:400]\n",
    "    \n",
    "    classification_results = train_with_different_batch_sizes(\n",
    "        X_clf_small, y_clf_small, X_clf_test_small, y_clf_test_small, 'classification')\n",
    "    \n",
    "else:\n",
    "    print(\"TensorFlow not available - skipping Keras examples\")\n",
    "    regression_results = {}\n",
    "    classification_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd7063",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9612c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "if TF_AVAILABLE and regression_results:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Batch vs Mini-Batch Training Comparison', fontsize=16)\n",
    "\n",
    "    # NumPy Loss Comparison\n",
    "    axes[0, 0].plot(history_batch['loss'], 'b-', label='Full Batch', linewidth=2)\n",
    "    axes[0, 0].plot(history_mini['loss'], 'r-', label='Mini-Batch (64)', linewidth=2)\n",
    "    axes[0, 0].plot(history_sgd['loss'], 'g-', label='SGD (size=1)', linewidth=2, alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('NumPy: Loss Convergence')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Keras Regression Loss Comparison\n",
    "    for name, result in regression_results.items():\n",
    "        axes[0, 1].plot(result['history'].history['loss'], label=name, linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title('TensorFlow/Keras: Regression Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Keras Classification Loss Comparison\n",
    "    for name, result in classification_results.items():\n",
    "        axes[0, 2].plot(result['history'].history['loss'], label=name, linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('Loss')\n",
    "    axes[0, 2].set_title('TensorFlow/Keras: Classification Loss')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Training Time Comparison\n",
    "    batch_names = list(regression_results.keys())\n",
    "    reg_times = [regression_results[name]['training_time'] for name in batch_names]\n",
    "    clf_times = [classification_results[name]['training_time'] for name in batch_names]\n",
    "\n",
    "    x_pos = np.arange(len(batch_names))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[1, 0].bar(x_pos - width/2, reg_times, width, label='Regression', alpha=0.7)\n",
    "    axes[1, 0].bar(x_pos + width/2, clf_times, width, label='Classification', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Batch Configuration')\n",
    "    axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "    axes[1, 0].set_title('Training Time Comparison')\n",
    "    axes[1, 0].set_xticks(x_pos)\n",
    "    axes[1, 0].set_xticklabels([name.replace('Batch Size ', '') for name in batch_names], rotation=45)\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # Memory Usage Simulation (Theoretical)\n",
    "    batch_sizes = [1, 16, 64, 256, 1024, len(X_small)]\n",
    "    memory_usage = [b * X_small.shape[1] * 4 / 1024 for b in batch_sizes]  # Approximate KB\n",
    "\n",
    "    axes[1, 1].semilogx(batch_sizes, memory_usage, 'o-', linewidth=2, markersize=8)\n",
    "    axes[1, 1].set_xlabel('Batch Size (log scale)')\n",
    "    axes[1, 1].set_ylabel('Memory Usage (KB)')\n",
    "    axes[1, 1].set_title('Memory Usage vs Batch Size')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Final Performance Comparison\n",
    "    final_losses_reg = [regression_results[name]['final_val_loss'] for name in batch_names]\n",
    "    final_losses_clf = [classification_results[name]['final_val_loss'] for name in batch_names]\n",
    "\n",
    "    axes[1, 2].bar(x_pos - width/2, final_losses_reg, width, label='Regression', alpha=0.7)\n",
    "    axes[1, 2].bar(x_pos + width/2, final_losses_clf, width, label='Classification', alpha=0.7)\n",
    "    axes[1, 2].set_xlabel('Batch Configuration')\n",
    "    axes[1, 2].set_ylabel('Final Validation Loss')\n",
    "    axes[1, 2].set_title('Final Performance Comparison')\n",
    "    axes[1, 2].set_xticks(x_pos)\n",
    "    axes[1, 2].set_xticklabels([name.replace('Batch Size ', '') for name in batch_names], rotation=45)\n",
    "    axes[1, 2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # Simplified visualization with just NumPy results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('NumPy: Batch vs Mini-Batch Comparison', fontsize=16)\n",
    "    \n",
    "    # Loss comparison\n",
    "    axes[0].plot(history_batch['loss'], 'b-', label='Full Batch', linewidth=2)\n",
    "    axes[0].plot(history_mini['loss'], 'r-', label='Mini-Batch (64)', linewidth=2)\n",
    "    axes[0].plot(history_sgd['loss'], 'g-', label='SGD (size=1)', linewidth=2, alpha=0.7)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss Convergence Comparison')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Memory usage\n",
    "    batch_sizes = [1, 32, 64, 128, 256, len(X_small)]\n",
    "    memory_usage = [b * X_small.shape[1] * 4 / 1024 for b in batch_sizes]\n",
    "    \n",
    "    axes[1].semilogx(batch_sizes, memory_usage, 'o-', linewidth=2, markersize=8)\n",
    "    axes[1].set_xlabel('Batch Size (log scale)')\n",
    "    axes[1].set_ylabel('Memory Usage (KB)')\n",
    "    axes[1].set_title('Memory Usage vs Batch Size')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53499a",
   "metadata": {},
   "source": [
    "## 6. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BATCH vs MINI-BATCH SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(\"1. CONVERGENCE SPEED:\")\n",
    "print(\"   • Full Batch: Slower per epoch, smoother convergence\")\n",
    "print(\"   • Mini-Batch: Faster per epoch, slightly noisy but effective\")\n",
    "print(\"   • SGD: Fastest per update, very noisy but can escape local minima\")\n",
    "\n",
    "print(\"\\n2. MEMORY EFFICIENCY:\")\n",
    "print(\"   • Full Batch: High memory usage, scales with dataset size\")\n",
    "print(\"   • Mini-Batch: Predictable memory usage, independent of dataset size\")\n",
    "print(\"   • SGD: Minimal memory usage\")\n",
    "\n",
    "print(\"\\n3. COMPUTATIONAL EFFICIENCY:\")\n",
    "print(\"   • Full Batch: Better GPU utilization but slower overall\")\n",
    "print(\"   • Mini-Batch: Good balance of speed and stability\")\n",
    "print(\"   • SGD: Poor GPU utilization due to small batch size\")\n",
    "\n",
    "print(\"\\nRECOMMENDED BATCH SIZES:\")\n",
    "print(\"• Small datasets (< 1000 samples): Full batch or large mini-batches\")\n",
    "print(\"• Medium datasets (1k-100k samples): Mini-batches of 32-128\")\n",
    "print(\"• Large datasets (> 100k samples): Mini-batches of 64-512\")\n",
    "print(\"• Very large datasets: Mini-batches of 256-1024\")\n",
    "\n",
    "print(\"\\nWHEN TO USE MINI-BATCHES:\")\n",
    "print(\"✓ Large datasets that don't fit in memory\")\n",
    "print(\"✓ Need faster training iterations\")\n",
    "print(\"✓ Want regularization effect from noise\")\n",
    "print(\"✓ GPU training (better parallelization)\")\n",
    "print(\"✓ Online learning scenarios\")\n",
    "\n",
    "print(\"\\nWHEN TO USE FULL BATCH:\")\n",
    "print(\"✓ Small datasets that fit in memory\")\n",
    "print(\"✓ Need most accurate gradient estimates\")\n",
    "print(\"✓ Convex optimization problems\")\n",
    "print(\"✓ Final fine-tuning stages\")\n",
    "\n",
    "if TF_AVAILABLE and regression_results:\n",
    "    print(\"\\nTENSORFLOW/KERAS RESULTS:\")\n",
    "    for name, result in regression_results.items():\n",
    "        print(f\"{name:15s} - Final Loss: {result['final_loss']:.4f}, Time: {result['training_time']:.2f}s\")\n",
    "\n",
    "print(\"\\nNUMPY RESULTS:\")\n",
    "print(f\"Full Batch      - Final Loss: {history_batch['loss'][-1]:.4f}\")\n",
    "print(f\"Mini-Batch (64) - Final Loss: {history_mini['loss'][-1]:.4f}\")\n",
    "print(f\"SGD (size=1)    - Final Loss: {history_sgd['loss'][-1]:.4f}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7cf4e",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Batch Training vs Mini-Batch Training\n",
    "\n",
    "1. **Memory Management**: Mini-batch training allows you to train on datasets larger than your available RAM\n",
    "2. **Training Speed**: Mini-batches provide faster iterations and often reach good solutions quicker\n",
    "3. **Generalization**: The noise in mini-batch gradients can act as regularization, potentially leading to better generalization\n",
    "4. **Hardware Utilization**: Mini-batches make better use of modern GPU architectures\n",
    "5. **Practical Implementation**: Most deep learning frameworks are optimized for mini-batch processing\n",
    "\n",
    "### Framework-Specific Considerations\n",
    "\n",
    "- **NumPy**: Great for understanding the fundamentals, but limited scalability\n",
    "- **Pandas**: Excellent for data manipulation and creating custom batching logic\n",
    "- **TensorFlow/Keras**: Production-ready with built-in batching, GPU support, and optimization\n",
    "- **PyTorch**: Similar to TensorFlow with more dynamic computation graphs\n",
    "\n",
    "The choice between batch and mini-batch training depends on your specific use case, dataset size, and computational resources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
