{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b9cf9c",
   "metadata": {},
   "source": [
    "# Components of Machine Learning\n",
    "\n",
    "This notebook demonstrates the **components of Machine Learning**:\n",
    "\n",
    "1. **Data** - Features and Labels\n",
    "2. **Hypothesis Space (Model)** - Both traditional ML and PyTorch Neural Networks\n",
    "3. **Loss Functions** - Optimization objectives\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c51f7346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Setup Complete!\n",
      "PyTorch version: 2.7.1+cu126\n",
      "CUDA available: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Traditional ML libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment Setup Complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a07c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Component: Data\n",
    "\n",
    "**Data** is the foundation of machine learning. It consists of:\n",
    "- **Features (X)**: Input variables or measurements\n",
    "- **Labels/Targets (y)**: What we want to predict\n",
    "\n",
    "### The Iris Dataset\n",
    "\n",
    "We'll use the famous Iris dataset which contains:\n",
    "- **150 samples** of iris flowers\n",
    "- **4 features**: sepal length, sepal width, petal length, petal width\n",
    "- **3 classes**: Setosa, Versicolor, Virginica\n",
    "\n",
    "This dataset is perfect for demonstrating both binary and multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55a02dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ¸ IRIS DATASET OVERVIEW\n",
      "========================================\n",
      "Dataset shape: (150, 4)\n",
      "Number of classes: 3\n",
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Class names: ['setosa' 'versicolor' 'virginica']\n",
      "Classes distribution: [50 50 50]\n",
      "\n",
      "First 5 samples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target species  \n",
       "0       0  setosa  \n",
       "1       0  setosa  \n",
       "2       0  setosa  \n",
       "3       0  setosa  \n",
       "4       0  setosa  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "\n",
    "print(\"ðŸŒ¸ IRIS DATASET OVERVIEW\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Feature names: {iris_data.feature_names}\")\n",
    "print(f\"Class names: {iris_data.target_names}\")\n",
    "print(f\"Classes distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=iris_data.feature_names)\n",
    "df['target'] = y\n",
    "df['species'] = [iris_data.target_names[i] for i in y]\n",
    "\n",
    "print(f\"\\nFirst 5 samples:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Scatter plot: Sepal dimensions\n",
    "sns.scatterplot(data=df, x='sepal length (cm)', y='sepal width (cm)', \n",
    "                hue='species', s=60, ax=axes[0,0])\n",
    "axes[0,0].set_title('Sepal Dimensions')\n",
    "\n",
    "# Scatter plot: Petal dimensions  \n",
    "sns.scatterplot(data=df, x='petal length (cm)', y='petal width (cm)',\n",
    "                hue='species', s=60, ax=axes[0,1])\n",
    "axes[0,1].set_title('Petal Dimensions')\n",
    "\n",
    "# Distribution plots\n",
    "for i, feature in enumerate(['sepal length (cm)', 'sepal width (cm)']):\n",
    "    sns.histplot(data=df, x=feature, hue='species', alpha=0.7, \n",
    "                 ax=axes[1,i], kde=True)\n",
    "    axes[1,i].set_title(f'{feature.title()} Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Setosa is clearly separable from the other two species\")\n",
    "print(\"- Versicolor and Virginica have some overlap\")  \n",
    "print(\"- Petal dimensions show better separation than sepal dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d252f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Component: Hypothesis Space (Models)\n",
    "\n",
    "The **hypothesis space** defines the set of all possible functions our model can represent. Let's compare two approaches:\n",
    "\n",
    "### Approach 1: Traditional Machine Learning (Scikit-learn)\n",
    "\n",
    "Traditional ML uses well-established algorithms with built-in assumptions:\n",
    "- **Logistic Regression**: Linear decision boundaries\n",
    "- **Random Forest**: Tree-based ensemble methods\n",
    "- **SVM**: Margin-based classification\n",
    "\n",
    "**Advantages**: Simple, fast, interpretable  \n",
    "**Disadvantages**: Limited complexity, requires feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional ML: Scikit-learn implementation\n",
    "print(\"TRADITIONAL MACHINE LEARNING (Scikit-learn)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Traditional ML models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results_traditional = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results_traditional[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Training: model.fit(X_train, y_train)\")\n",
    "    print(f\"  Prediction: model.predict(X_test)\")\n",
    "\n",
    "print(f\"\\nScikit-learn makes ML simple with just 2 lines of code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6471e",
   "metadata": {},
   "source": [
    "### Approach 2: Deep Learning (PyTorch)\n",
    "\n",
    "PyTorch allows us to build **custom neural networks** with complete control over:\n",
    "- **Architecture**: Number of layers, neurons, activations\n",
    "- **Training Process**: Optimizers, learning rates, batch sizes\n",
    "- **Loss Functions**: Custom objectives for specific tasks\n",
    "\n",
    "**Advantages**: Extremely flexible, can learn complex patterns  \n",
    "**Disadvantages**: More complex, requires more data and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Neural Network Implementation\n",
    "print(\"DEEP LEARNING (PyTorch)\")  \n",
    "print(\"=\" * 30)\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"PyTorch Neural Network with multiple hidden layers\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=4, hidden_dim=64, num_classes=3):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),                          # Non-linearity\n",
    "            nn.Dropout(0.2),                    # Regularization\n",
    "            \n",
    "            # Second hidden layer\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(hidden_dim//2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "# Create model instance\n",
    "model = MultiLayerPerceptron(input_dim=4, hidden_dim=64, num_classes=3)\n",
    "print(f\"Model Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare PyTorch data\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Loss Function: {criterion}\")\n",
    "print(f\"  Optimizer: Adam (lr=0.01)\")\n",
    "print(f\"  Batch Size: 16\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d96fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "print(\"\\nStarting PyTorch training.\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set to training mode\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update parameters (Adam magic!)\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += batch_y.size(0)\n",
    "        correct_predictions += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = 100 * correct_predictions / total_samples\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1:3d}/{num_epochs}] | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e9b44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Component: Loss Functions\n",
    "\n",
    "**Loss functions** measure how wrong our predictions are and guide the learning process. Different tasks require different loss functions:\n",
    "\n",
    "### Classification Loss Functions\n",
    "\n",
    "1. **CrossEntropyLoss** - Multi-class classification\n",
    "   - Formula: `CE = -Î£(y_true * log(y_pred))`\n",
    "   - Used when predicting among multiple classes\n",
    "\n",
    "2. **BCELoss** - Binary classification\n",
    "   - Formula: `BCE = -[y*log(p) + (1-y)*log(1-p)]`\n",
    "   - Used for yes/no, true/false predictions\n",
    "\n",
    "### Regression Loss Functions\n",
    "\n",
    "3. **MSELoss** - Mean Squared Error\n",
    "   - Formula: `MSE = (1/n) * Î£(y_true - y_pred)Â²`\n",
    "   - Penalizes large errors heavily\n",
    "\n",
    "4. **L1Loss** - Mean Absolute Error\n",
    "   - Formula: `MAE = (1/n) * Î£|y_true - y_pred|`\n",
    "   - More robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different PyTorch loss functions\n",
    "print(\"PYTORCH LOSS FUNCTIONS DEMO\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Sample data for demonstration\n",
    "y_true_class = torch.tensor([0, 1, 2, 1, 0])  # True class labels\n",
    "y_pred_logits = torch.tensor([\n",
    "    [2.0, -1.0, 0.5],   # Prediction for sample 1\n",
    "    [-0.5, 1.5, 0.2],   # Prediction for sample 2\n",
    "    [0.1, -0.8, 1.2],   # Prediction for sample 3\n",
    "    [0.3, 0.9, -0.1],   # Prediction for sample 4\n",
    "    [1.8, -0.2, 0.0]    # Prediction for sample 5\n",
    "])\n",
    "\n",
    "# Convert logits to probabilities\n",
    "y_pred_probs = torch.softmax(y_pred_logits, dim=1)\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "print(f\"True labels: {y_true_class.tolist()}\")\n",
    "print(f\"Predicted probabilities shape: {y_pred_probs.shape}\")\n",
    "print(f\"Predicted probabilities:\\n{y_pred_probs}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# 1. Cross Entropy Loss (for classification)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "ce_value = ce_loss(y_pred_logits, y_true_class)\n",
    "print(f\"1. CrossEntropyLoss: {ce_value:.4f}\")\n",
    "print(\"   â†³ Used for multi-class classification\")\n",
    "print(\"   â†³ Takes raw logits (no softmax needed)\")\n",
    "\n",
    "# 2. Negative Log Likelihood Loss\n",
    "nll_loss = nn.NLLLoss()\n",
    "nll_value = nll_loss(torch.log(y_pred_probs), y_true_class)\n",
    "print(f\"\\n2. NLLLoss: {nll_value:.4f}\")\n",
    "print(\"   â†³ CrossEntropy = Softmax + NLLLoss\")\n",
    "\n",
    "# 3. Mean Squared Error (for regression)\n",
    "y_true_reg = torch.tensor([1.0, 2.5, 0.8, 1.9, 0.3])\n",
    "y_pred_reg = torch.tensor([1.1, 2.3, 0.9, 2.1, 0.1])\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "mse_value = mse_loss(y_pred_reg, y_true_reg)\n",
    "print(f\"\\n3. MSELoss: {mse_value:.4f}\")\n",
    "print(\"   â†³ Used for regression tasks\")\n",
    "print(\"   â†³ Squares the differences (penalizes large errors)\")\n",
    "\n",
    "# 4. Mean Absolute Error (for regression)\n",
    "mae_loss = nn.L1Loss()\n",
    "mae_value = mae_loss(y_pred_reg, y_true_reg)\n",
    "print(f\"\\n4. L1Loss (MAE): {mae_value:.4f}\")\n",
    "print(\"   â†³ Used for regression tasks\")\n",
    "print(\"   â†³ Less sensitive to outliers than MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PyTorch model\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor.to(device))\n",
    "    _, test_predictions = torch.max(test_outputs, 1)\n",
    "    test_predictions = test_predictions.cpu().numpy()\n",
    "    \n",
    "    pytorch_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"PyTorch MLP Accuracy: {pytorch_accuracy:.4f}\")\n",
    "\n",
    "# Compare all approaches\n",
    "print(\"\\nðŸ“Š FINAL COMPARISON\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Logistic Regression: {results_traditional['Logistic Regression']['accuracy']:.4f}\")\n",
    "print(f\"Random Forest:       {results_traditional['Random Forest']['accuracy']:.4f}\")\n",
    "print(f\"PyTorch MLP:         {pytorch_accuracy:.4f}\")\n",
    "\n",
    "# Determine best model\n",
    "best_score = max(\n",
    "    results_traditional['Logistic Regression']['accuracy'],\n",
    "    results_traditional['Random Forest']['accuracy'],\n",
    "    pytorch_accuracy\n",
    ")\n",
    "\n",
    "print(f\"\\nBest performing model: {best_score:.4f} accuracy\")\n",
    "\n",
    "if best_score == pytorch_accuracy:\n",
    "    print(\"   Winner: PyTorch MLP\")\n",
    "elif best_score == results_traditional['Random Forest']['accuracy']:\n",
    "    print(\"   Winner: Random Forest\")\n",
    "else:\n",
    "    print(\"   Winner: Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa360bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualizations\n",
    "print(\"VISUALIZATIONS\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. PyTorch Training History - Loss\n",
    "axes[0, 0].plot(train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "axes[0, 0].set_title('PyTorch Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('CrossEntropy Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. PyTorch Training History - Accuracy\n",
    "axes[0, 1].plot(train_accuracies, 'g-', linewidth=2, label='Training Accuracy')\n",
    "axes[0, 1].set_title('PyTorch Training Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Model Comparison Bar Chart\n",
    "models_names = ['Logistic Reg', 'Random Forest', 'PyTorch MLP']\n",
    "accuracies = [\n",
    "    results_traditional['Logistic Regression']['accuracy'],\n",
    "    results_traditional['Random Forest']['accuracy'], \n",
    "    pytorch_accuracy\n",
    "]\n",
    "colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "\n",
    "bars = axes[0, 2].bar(models_names, accuracies, color=colors, alpha=0.8)\n",
    "axes[0, 2].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Accuracy')\n",
    "axes[0, 2].set_ylim(0.8, 1.0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Confusion Matrix for PyTorch\n",
    "cm_pytorch = confusion_matrix(y_test, test_predictions)\n",
    "sns.heatmap(cm_pytorch, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris_data.target_names,\n",
    "            yticklabels=iris_data.target_names,\n",
    "            ax=axes[1, 0])\n",
    "axes[1, 0].set_title('PyTorch MLP - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "\n",
    "# 5. Feature Importance Visualization (Random Forest)\n",
    "feature_importance = results_traditional['Random Forest']['model'].feature_importances_\n",
    "axes[1, 1].barh(iris_data.feature_names, feature_importance, color='lightcoral')\n",
    "axes[1, 1].set_title('Random Forest - Feature Importance', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Importance')\n",
    "\n",
    "# 6. Loss Function Comparison\n",
    "loss_names = ['CrossEntropy', 'NLL', 'MSE', 'L1/MAE']\n",
    "loss_values = [ce_value.item(), nll_value.item(), mse_value.item(), mae_value.item()]\n",
    "loss_colors = ['red', 'orange', 'purple', 'brown']\n",
    "\n",
    "axes[1, 2].bar(loss_names, loss_values, color=loss_colors, alpha=0.7)\n",
    "axes[1, 2].set_title('PyTorch Loss Functions', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_ylabel('Loss Value')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for i, (name, value) in enumerate(zip(loss_names, loss_values)):\n",
    "    axes[1, 2].text(i, value + 0.05, f'{value:.3f}', \n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"All visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf718c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### The 3 Components of Machine Learning\n",
    "\n",
    "| Component | Traditional ML | Deep Learning (PyTorch) |\n",
    "|-----------|---------------|------------------------|\n",
    "| **Data** | Features + Labels | Tensors + DataLoaders |\n",
    "| **Model** | `model.fit()` | Custom `nn.Module` classes |\n",
    "| **Loss** | Built-in metrics | Flexible loss functions |\n",
    "\n",
    "### **When to use traditional ML or Deep Learning and PyTorch?**\n",
    "\n",
    "**Traditional ML (Scikit-learn):**\n",
    "- âœ… Small datasets (< 10,000 samples)\n",
    "- âœ… Tabular data\n",
    "- âœ… Need interpretability\n",
    "- âœ… Quick prototyping\n",
    "- âœ… Limited computational resources\n",
    "\n",
    "**Deep Learning (PyTorch):**\n",
    "- âœ… Large datasets (> 100,000 samples)\n",
    "- âœ… Complex patterns (images, text, audio)\n",
    "- âœ… Need custom architectures\n",
    "- âœ… GPU resources available\n",
    "\n",
    "### **PyTorch concepts**\n",
    "\n",
    "1. **Adam Optimizer**: Adaptive learning rates, momentum, bias correction\n",
    "2. **Neural Architecture**: Linear layers + ReLU activations + Dropout\n",
    "3. **Loss Functions**: CrossEntropy (classification), MSE (regression)\n",
    "4. **Training Loop**: Forward pass â†’ Loss â†’ Backward pass â†’ Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee46131",
   "metadata": {},
   "source": [
    "### **Next steps**\n",
    "\n",
    "1. **Architecture**:\n",
    "   - Change `hidden_dim` from 64 to 32 or 128\n",
    "   - Add more layers to the neural network\n",
    "   - Try different activation functions (`nn.Tanh`, `nn.LeakyReLU`)\n",
    "\n",
    "2. **Optimizer**:\n",
    "   - Replace Adam with `optim.SGD(model.parameters(), lr=0.1)`\n",
    "   - Try different learning rates: 0.001, 0.01, 0.1\n",
    "\n",
    "3. **Loss Function**:\n",
    "   - For binary classification, use `nn.BCEWithLogitsLoss()`\n",
    "   - For regression tasks, try `nn.L1Loss()` instead of `nn.MSELoss()`\n",
    "\n",
    "4. **Data**:\n",
    "   - Try other sklearn datasets: `load_wine()`, `load_breast_cancer()`\n",
    "   - Experiment with different train/test split ratios\n",
    "\n",
    "### **Resources**\n",
    "\n",
    "- **PyTorch Tutorials**: https://pytorch.org/tutorials/\n",
    "- **Deep Learning Book**: http://www.deeplearningbook.org/\n",
    "- **Fast.ai Course**: https://www.fast.ai/\n",
    "- **PyTorch Documentation**: https://pytorch.org/docs/\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
